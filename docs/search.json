[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I specialize in the transportation sector, particularly focusing on electric vehicles, including lithium-ion batteries and solar panels. In my leisure time, I enjoy watching and playing soccer."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Portfolio",
    "section": "",
    "text": "Impact of Vehicle Types on GHG Emissions\n\n\n\nR\n\n\nVisualize\n\n\nMEDS\n\n\n\nDo vehicle manufacturers make efforts to reduce greenhouse gas emissions?\n\n\n\nHaejin Kim\n\n\nMar 19, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVehicles emission affects to the climate change\n\n\n\nR\n\n\nStatistics\n\n\nMEDS\n\n\n\nImpact of CAFE standards on reducing carbon dioxide emissions and compare truck and car models relative to these standards.\n\n\n\nHaejin Kim\n\n\nDec 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMapping the global potential for marine aquaculture\n\n\n\nGeospatial\n\n\nR\n\n\nMEDS\n\n\n\nMapping global marine aquaculture potential, particularly on the West Coast of the US, focusing on optimal oyster cultivation in Exclusive Economic Zones.\n\n\n\nHaejin Kim\n\n\nDec 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAir Quality Analysis of Thomas Fire\n\n\n\nGeospetial\n\n\nData_Science\n\n\nMEDS\n\n\n\nVisualize the impact of the Thomas Fire on air quality in Santa Barbara County, California\n\n\n\nHaejin Kim\n\n\nDec 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Accessibility in North Korea\n\n\n\nNorth_Korea\n\n\nData_Science\n\n\nMEDS\n\n\n\nPolitical Challenges and Environment Data Accessibility in North Korea\n\n\n\nHaejin Kim\n\n\nDec 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEarthquake\n\n\n\nQGIS\n\n\n\nEarthquake Hazard of San Francisco Bay Area\n\n\n\nHaejin Kim\n\n\nJul 30, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-12-08-ethics/index.html",
    "href": "posts/2023-12-08-ethics/index.html",
    "title": "Data Accessibility in North Korea",
    "section": "",
    "text": "The yearning for reunification holds great importance for many Koreans, a desire to bridge the gap caused by separation. During the Korean War, my grandparents’ move from North to South Korea resulted in the heartbreaking disconnection from our relatives. Despite the passage of time, political barriers have left us unable to reach to their family in the North, leaving a void of information about their well-being. The difficulty in obtaining any updates from North Korea intensifies our distress, leaving us in the dark about the lives of our loved ones there.\nEnvironmental Challenges Amid Political Complexity While prioritizing environmental issues is a consensus shared by many globally, it might not resonate similarly in Least Developed Countries (LDCs), Small Island Developing States (SIDs), or several African nations. Their existence is deeply entwined with adversity—war, famine, hunger, and poverty—which overshadows environmental concerns. Consequently, obtaining comprehensive data resources from these countries remains an ongoing struggle. North Korea faces a similar plight, grappling with hunger, famine, and a high mortality rate stemming from these issues. Expecting them to prioritize collecting environmental data becomes an impractical demand.\nRecent reports of flooding in North Korea resulted in significant damage to their crops, exacerbating deforestation caused by logging and cultivation. They are critically exposed to hunger and famine. Their ability to engage in sustainable activities has been severely hindered by environmental disasters. They are in dire need of substantial aid from other nations to ensure food security. However, the absence of adequate data makes it challenging to predict the extent of their suffering and devise effective assistance strategies. [1]\n\nTransparent Environmental Data\nThe fields of data science and the environment should be open sources because environmental issues transcend borders and require global protection. Yet, this poses a sensitive matter, with private technologies reluctant to relinquish profits by divulging crucial data. Acknowledging missing data is crucial, exemplified by the “The pentagon, climate change, and war” by Crawford, gleaned from lectures such as the one illustrating the US military’s efforts toward carbon reduction.\nDespite the necessity for most data to be transparent in safeguarding the environment, numerous countries withhold information due to political motives. Countries like China, Russia, and notably North Korea have a significant impact on global environmental dynamics, yet their data remains undisclosed due to political agendas. While working at the Green Climate Fund, I delved into various nations’ National Designated Authority (NDC) reports on reducing greenhouse gas emissions. Although some countries, particularly smaller ones like island nations, demonstrate optimistic goals for emission reduction through renewable energy and electric vehicles, China, Russia, and notably North Korea, present challenges in obtaining crucial environmental data.\n\nSatellite data has been instrumental in acquiring insights into North Korea’s forests and land use. However, much of this data remains concealed due to political constraints. During my studies of climate change law at Columbia University, I realized the extent to which environmental science intertwines with legal frameworks, emphasizing the role of political decisions in environmental matters. North Korea faces a similar scenario, heavily reliant on government decisions and propaganda, omitting itself even from the LDCs categories, complicating efforts to assist the country due to a lack of comprehensive data sources.\nStrategies for Addressing North Korea’s Environmental Challenges\nIn light of these challenges, determining North Korea’s environmental priorities to combat climate change and avert future crises becomes imperative. Despite the scarcity of data, information from sources like the offers some insight, albeit limited to specific areas, potentially leading to biased data due to limited resources. Overall, the need for reunification in Korea remains a poignant hope, while addressing environmental concerns in North Korea requires a concerted effort to access vital data and devise strategies that transcend political barriers for a sustainable future.\nHow to overcome this situation? To navigate situations akin to North Korea’s challenges, looking at comparable cases like Myanmar and Eritrea offers valuable insights. Myanmar, once under prolonged political isolation like to North Korea, underwent political reforms that improved transparency, enabling better access to environmental data. This shift allowed for enhanced monitoring and conservation efforts, highlighting the positive impact of political openness. Similarly, Eritrea faced isolation due to political factors, hindering environmental solutions. However, partnerships with international agencies and improved collaboration with neighbors have gradually enabled better environmental monitoring and resource management. These examples emphasize the pivotal role of political reforms, international collaboration, and transparency in overcoming limitations to address environmental challenges in politically isolated nations.\nWhile Myanmar and Eritrea exemplify progress achieved through political reforms and international collaboration to address environmental challenges, North Korea’s situation remains distinct due to its prolonged political isolation. Unlike Myanmar’s recent reforms or Eritrea’s partnerships, North Korea’s closed-off regime presents a more complex scenario. The lack of transparent access to environmental data in North Korea hinders global efforts to comprehensively tackle environmental concerns. [3][4]\nUnique Challenges in North Korea\nHowever, drawing from successful strategies applied elsewhere, such as Myanmar and Eritrea, provides a blueprint for potential avenues in addressing North Korea’s environmental issues. Engaging in diplomatic dialogues and fostering international partnerships could facilitate access to crucial environmental data. International organizations could play a pivotal role in encouraging transparency and advocating for information sharing on environmental matters, thereby aiding North Korea’s efforts to monitor and address environmental challenges. By leveraging lessons learned from comparable cases and employing innovative diplomatic approaches, there is potential to navigate the unique challenges North Korea presents in addressing its environmental issues.\nData Reconciliation Post-Reunification\nFurthermore, the potential reunion of North Korea and South Korea raises several critical considerations regarding environmental issues and data collection. Achieving data reconciliation between the two Koreas post-reunification would present a significant challenge requiring a multifaceted approach. Essential aspects would involve aligning disparate data systems, fostering transparency, and encouraging information exchange. Establishing standardized protocols, investing in capacity development, seeking international collaboration, and managing transitional phases would be pivotal. However, the success of this complex undertaking would heavily rely on political willingness, diplomatic efforts, and a gradual integration process that acknowledges the intricacies of merging distinct data frameworks. Resolving data disparities demands a comprehensive strategy entwining technological progress, collaborative institutions, and diplomatic initiatives to facilitate unified and coherent data practices following reunification.\n\nIn conclusion, the quest for reunification in Korea holds immense significance, coupled with the urgency to address environmental concerns in North Korea. The emotional toll of separation persists, compounded by political barriers that sever ties with relatives in the North. Despite global consensus on environmental priorities, challenges in obtaining comprehensive data persist, hindering effective solutions. The need for transparency in data sharing for global environmental protection is evident, yet political motives impede progress. Strategies derived from comparable cases offer potential avenues for addressing North Korea’s environmental issues. Moreover, the complexity of data reconciliation post-reunification necessitates multifaceted approaches, emphasizing the crucial role of political willingness and international collaboration. To forge a sustainable future and navigate North Korea’s unique challenges, concerted efforts bridging political divides and technological advancements are imperative.\nReference\n[1] North Korea’s Kim blasts ‘irresponsible’ top officials for flood damage: https://www.reuters.com/world/asia-pacific/north-koreas-kim-blasts-irresponsible-top-officials-flood-damage-2023-08-22/\n[2] The pentagon, climate change and war, Crawford pp.103 - 177\n[3] https://hir.harvard.edu/why-has-the-world-forgotten-about-myanmar/\n[4] https://er.usembassy.gov/our-relationship/policy-history/\n\n\n\nCitationBibTeX citation:@online{kim2023,\n  author = {Kim, Haejin},\n  title = {Data {Accessibility} in {North} {Korea}},\n  date = {2023-12-09},\n  url = {https://khj9759.github.io/posts/2023-12-09-ethics/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nKim, Haejin. 2023. “Data Accessibility in North Korea.”\nDecember 9, 2023. https://khj9759.github.io/posts/2023-12-09-ethics/."
  },
  {
    "objectID": "posts/2023-12-9-transport-emission/index.html",
    "href": "posts/2023-12-9-transport-emission/index.html",
    "title": "Vehicles emission affects to the climate change",
    "section": "",
    "text": "Repository: https://github.com/khj9759/transport_emission"
  },
  {
    "objectID": "posts/2023-12-9-transport-emission/index.html#import-libraries",
    "href": "posts/2023-12-9-transport-emission/index.html#import-libraries",
    "title": "Vehicles emission affects to the climate change",
    "section": "Import libraries",
    "text": "Import libraries\n\n\nCode\nlibrary(broom)\nlibrary(readr)\nlibrary(gt)\nlibrary(modelr)\nlibrary(lubridate)\nlibrary(xtable)\nlibrary(dplyr)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(tsibble)\nlibrary(sjPlot)\nlibrary(knitr)\nlibrary(here)\nlibrary(ggpubr)\nlibrary(jtools)\n\n\nrm(list = ls())\n\n\nhere::i_am(\"index.qmd\")\n\noptions(scipen = 999) # disable scientific notation"
  },
  {
    "objectID": "posts/2023-12-9-transport-emission/index.html#import-data-and-clean-up",
    "href": "posts/2023-12-9-transport-emission/index.html#import-data-and-clean-up",
    "title": "Vehicles emission affects to the climate change",
    "section": "Import Data and Clean up",
    "text": "Import Data and Clean up\n\n\nCode\n# ==============earth surface temperature data===============\ntemperature &lt;- read_csv(\"data/temperature_fig-1.csv\")\ncolnames(temperature) &lt;- c(\"year\", \"temp\", \"temp2\", \"temp3\")\n\n# Remove rows 1 to 6\ntemperature &lt;- temperature[-c(1:6), ]\n\n# Change row 7 to become row 1\nrow_to_move &lt;- temperature[7, ]\ntemperature &lt;- rbind(row_to_move, temperature[-7, ])\n\ntemperature$year &lt;- as.numeric(temperature$year)\ntemperature$temp &lt;- as.numeric(temperature$temp)\n\n# ==========vehicle information including co2 emission each car by year===============\nvehicle_info &lt;- read_csv(\"data/vehicle_info.csv\")\n\nnames(vehicle_info) &lt;- gsub(\" \", \"_\", names(vehicle_info))\nnames(vehicle_info) &lt;- gsub(\"-\", \"_\", names(vehicle_info))\nnames(vehicle_info) &lt;- gsub(\"/\", \"_\", names(vehicle_info))\nnames(vehicle_info) &lt;- gsub(\"[()]\", \"\", names(vehicle_info))\nvehicle_info$Model_Year &lt;- gsub(\"Prelim\\\\.\\\\s*\", \"\", vehicle_info$Model_Year)\nnames(vehicle_info)\n\n\n [1] \"Model_Year\"               \"Regulatory_Class\"        \n [3] \"Vehicle_Type\"             \"Production_Share\"        \n [5] \"Real_World_MPG\"           \"Real_World_MPG_City\"     \n [7] \"Real_World_MPG_Hwy\"       \"Real_World_CO2_g_mi\"     \n [9] \"Real_World_CO2_City_g_mi\" \"Real_World_CO2_Hwy_g_mi\" \n[11] \"Weight_lbs\"               \"Horsepower_HP\"           \n[13] \"Footprint_sq._ft.\"       \n\n\nCode\n# Function to check if a column can be converted to numeric\nis_numeric &lt;- function(x) {\n  all(!is.na(as.numeric(x)))\n}\n\n# Convert columns with numeric values as characters to numeric type, keep others as character\nvehicle_info &lt;- vehicle_info %&gt;%\n  mutate_if(~is.character(.) && all(sapply(., is_numeric)), as.numeric, na.rm = TRUE)\n\ncolnames(vehicle_info)[colnames(vehicle_info) == \"Model_Year\"] &lt;- \"year\"\n\nvehicle &lt;- left_join(temperature, vehicle_info, by = join_by(year))\n\n# ==========Accumulate emission===============\ncumulative_emissions &lt;- read_csv(\"data/cumulative-co-emissions.csv\")\n\nco2 &lt;- cumulative_emissions %&gt;%\n  filter(Entity == \"United States\") \n\ncolnames(co2) &lt;- c(\"nation\", \"nation1\", \"year\", \"accu_emission\") \n\nemission &lt;- left_join(vehicle, co2, by = join_by(year))\n\n# ==========Accumulate emission===============\ntransport_emission_rate &lt;- read_csv(\"data/emission_percent_transport.csv\")\ncolnames(transport_emission_rate) &lt;- c(\"year\", \"trp_emission_rate\")\n\ntransport_emission &lt;- full_join(emission, transport_emission_rate, by = join_by(year))\n\n## ========== count car ================\ncount_car &lt;- read_csv(\"data/car2.csv\")\n\n# Estimate the number of cars for years ranging from 1900 to 2022 using interpolation\nestimated_cars &lt;- approx(count_car$year, count_car$car, xout = 1900:2022)$y \n\nestimated_cars_data &lt;- data.frame(year = 1900:2022, number_of_car = estimated_cars)\n\nestimate_n_cars &lt;- full_join(transport_emission, estimated_cars_data, by = join_by(year))\n\npercent_emission &lt;- estimate_n_cars %&gt;% \n  mutate(transport_emission = accu_emission*trp_emission_rate*0.01)\n\nfinal &lt;- percent_emission %&gt;%      \n       mutate(emission_n_car = transport_emission/number_of_car) %&gt;% subset(year &gt;= 1975)\n\n## ================ Filter data starting from the year 1975 ===============\nfinal_subset &lt;- final %&gt;%\n  group_by(Regulatory_Class, Vehicle_Type) %&gt;% \n  filter(Regulatory_Class == \"All\", Vehicle_Type == \"All\")"
  },
  {
    "objectID": "posts/2023-12-9-transport-emission/index.html#data-explore",
    "href": "posts/2023-12-9-transport-emission/index.html#data-explore",
    "title": "Vehicles emission affects to the climate change",
    "section": "Data explore",
    "text": "Data explore\n\nEarth Surface Temperature: This dataset, sourced from the EPA, provides information on the surface temperature, specifically focusing on Ferrnite.\nVehicle: Within EPA dataset, various pieces of information are available.\n\nReal_World_CO2_g_mi: This metric represents the emissions per car and is crucial for assessing the efficiency of vehicles in reducing CO2 emissions.\nRegulatory_Class: This classification helps determine whether a vehicle is a car or a truck.\n\nAccumulative Emission: This data originated from ‘Our World in Data’\nTransportation Emission: This dataset, obtained from the WorldBank, sheds light on emissions related to transportation.\nNumber of Vehicles: Unfortunately, there is no dataset available on the current website. However, projections for the years 1900, 1901, 2020, 2021, 2022, and 2023 have been included in this dataset. (Data availability)"
  },
  {
    "objectID": "posts/2023-12-9-transport-emission/index.html#highlights",
    "href": "posts/2023-12-9-transport-emission/index.html#highlights",
    "title": "Vehicles emission affects to the climate change",
    "section": "Highlights",
    "text": "Highlights\n\nLinear regression\nMulti-linear regression\nInteraction model\nLogit regression\nNull Hypothesis"
  },
  {
    "objectID": "posts/2023-12-9-transport-emission/index.html#analysis",
    "href": "posts/2023-12-9-transport-emission/index.html#analysis",
    "title": "Vehicles emission affects to the climate change",
    "section": "Analysis",
    "text": "Analysis\n\nLinear Regression Model: Earth’s Surface Temperature and Emissions from Transport\nThe linear regression model examining the relationship between Earth’s surface temperature and emissions from transport is represented as:\n\\[\\text{Ave. Earth surface Temperature} = \\beta_0 + \\beta_1 \\text{emisson from transport} + \\epsilon\\]\n\nPlot\n\n\nCode\n# Create a line plot to visualize the relationship between transportation emissions and Earth surface temperature over the years\n\n# Define the plot using ggplot\ntemp_per_emssion_by_year &lt;- ggplot(final, aes(x = transport_emission, y = temp)) +\n  geom_line() +\n  geom_smooth(linetype = \"dashed\") +\n  labs(x = \"Transportation Emission (CO2e tonne)\", y = \"Earth Surface Temperature (F)\", title = \"Emission per Temperature (from 1975)\") +\n  theme_minimal()\n\n# Display the plot\ntemp_per_emssion_by_year\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 57 rows containing non-finite values (`stat_smooth()`).\n\n\nWarning: Removed 57 rows containing missing values (`geom_line()`).\n\n\n\n\n\nThese findings suggest that around 32% of the variability in transportation emissions can be explained by Earth surface temperature. This aligns with our scatter plot, depicting a consistently strong positive relationship. The favorable F-statistics and low p-values further indicate that the model provides a good fit to the data. Due to low \\(R^2\\) , it need to find the better fit.\n\n\nCode\n# Fit a linear regression model\nmodel &lt;- lm(temp ~ transport_emission, data = final_subset)\n\n# Display a summary of the regression analysis\nsumm(model)\n\n\n\n\n\n\nObservations\n40 (7 missing obs. deleted)\n\n\nDependent variable\ntemp\n\n\nType\nOLS linear regression\n\n\n\n\n\n\n\n\nF(1,38)\n17.96\n\n\nR²\n0.32\n\n\nAdj. R²\n0.30\n\n\n\n\n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n-1.26\n0.48\n-2.65\n0.01\n\n\ntransport_emission\n0.00\n0.00\n4.24\n0.00\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\nThis seems to exhibit a slightly better fit, as indicated by the higher \\(R^2\\) and adjusted \\(R^2\\) values. The Earth’s temperature increases with the rise in emissions from the transportation sector. Notably, there are significant errors associated with omitted variables, such as emissions from other sectors, including industry and power plants. Additionally, the scope needs to expand beyond the USA to encompass a global perspective.\n\n\nCode\n# Fit a linear regression model\nmodel_log &lt;- lm(temp ~ log(transport_emission), data = final_subset)\n\n# Display a summary of the regression analysis\nsumm(model_log)\n\n\n\n\n\n\nObservations\n40 (7 missing obs. deleted)\n\n\nDependent variable\ntemp\n\n\nType\nOLS linear regression\n\n\n\n\n\n\n\n\nF(1,38)\n19.98\n\n\nR²\n0.34\n\n\nAdj. R²\n0.33\n\n\n\n\n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n-48.56\n11.02\n-4.41\n0.00\n\n\nlog(transport_emission)\n1.96\n0.44\n4.47\n0.00\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression Model: Emission per car and Year since Regulation\nThis model defines the how CAFE regulation effects to the transportation emission. This model explains the vehicle emission effects on CAFE regulation with number of vehicle.\n\\[\\text{Emission per car} = \\beta_0 + \\beta_2 \\text{Year since Regulation} + \\epsilon\\]\nIn this plot, emission per car decreasing since regulation started. There was no omitted variable in this model. Based on this information, we would like to how regulation affects on overall transport emission. This model proves that the model shows the interaction between the regulation and number of the car affects on the emission per car. Year since regulation and number of car are independent variables, but it is depended on emission per car variable.\n\n\nCode\n# Create a line plot using ggplot for 'emission_per_car' from the filtered data\none_car_per_emission&lt;- ggplot(final_subset, aes(x = year, y = Real_World_CO2_g_mi)) +\n  geom_jitter() +\n  geom_smooth(linetype = \"dashed\") +\n  labs(x = \"Year\",\n       y = \"Emission Economy (g/mi)\",\n       title = \"emission per car by year\") +\n  theme_minimal()\n\none_car_per_emission\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\nModel without Interaction: Total CO2 emission saving\n\\[\\text{emission per car} = \\beta_0 + \\beta_2 \\text{year since regulation} + \\beta_3 \\text{number of car} + \\epsilon\\]\n\\(\\beta_0\\): The predicted emission per car before the regulation is 476 grams per mile (mi/g).\n\\(\\beta_2\\): The emissions per car decrease by 5 grams per mile for every year since the regulation, holding the number of cars constant. The coefficient β₂ can be interpreted as the effect of an additional year since the regulation was implemented on emission savings or reduction of emission.\n\\(\\beta_3\\): This coefficient represents the estimated change in emission per car for each additional unit in the “number of car” variable while holding “year since regulation” constant. It implies that as the number of cars increases, overall emission is increasing.\n\nPlot\n\n\nCode\n# Create a scatter plot with a regression line\nggplot(final_subset, aes(x = year, y = Real_World_CO2_g_mi)) +\n  geom_jitter() +  # Scatter plot\n  geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE, color = \"blue\") +  # Regression line\n  labs(x = \"Year Since Regulation\", y = \"Emission Economy (g/mi)\", title = \"Total CO2 emission saving\")\n\n\n\n\n\nIn this analysis, p-value is 9.1e-9, which is &lt; 0.05 can reject null hypothesis. R square is pretty low, which is 0.5.\n\n\nCode\n## Convert 'year' to (year - 1974): Year in 1975 becomes 1.\nfinal_subset$years &lt;- final_subset$year - 1974\n\n## Fit a linear model without interaction\nmodel_not_int &lt;- lm(Real_World_CO2_g_mi ~ years + number_of_car, data = final_subset)\nsumm(model_not_int, digits = 10)\n\n\n\n\n\n\nObservations\n47\n\n\nDependent variable\nReal_World_CO2_g_mi\n\n\nType\nOLS linear regression\n\n\n\n\n\n\n\n\nF(2,44)\n23.9688718881\n\n\nR²\n0.5214152731\n\n\nAdj. R²\n0.4996614218\n\n\n\n\n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n476.2446425522\n111.8001104335\n4.2597868706\n0.0001060282\n\n\nyears\n-5.1880489159\n3.8176342030\n-1.3589696236\n0.1810818273\n\n\nnumber_of_car\n0.0000000533\n0.0000001314\n0.4057541837\n0.6868896864\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInteraction model: Total CO2 emission saving\n\\[\\text{emission per car} = \\beta_0 + \\beta_2 \\text{year since regulation} + \\beta_3 \\text{number of car} +\\beta_4 \\text{year since regulation x number of car} + \\epsilon\\]\n\\(\\beta_4\\): This is interaction term implies the influence of the number of car on emission per car slightly changes as the year since regulation starts and it is defined the barometer of total CO2 emission saving.\n\n\nCode\n# Create a scatter plot with a regression line\nggplot(final_subset, aes(x = year, y = Real_World_CO2_g_mi)) +\n  geom_jitter() +  # Scatter plot\n  geom_smooth(method = \"lm\", formula = y ~ x, se = FALSE, color = \"blue\") +  # Regression line\n  labs(x = \"Year Since Regulation\", y = \"Emission Economy (g/mi)\", title = \"Total CO2 emission saving\") +\n  geom_abline(intercept = 690, slope = 0, linetype = \"solid\", color = \"red\")   # Red straight line\n\n\n\n\n\nIf there were no CAFE regulations since 1975, the red line in the plot would appear differently, assuming a steeper slope. Below the red line, it signifies the reduction in CO2 emissions as the number of cars increases. Even though the slope is 6.8e-9, it is telling that some of reduction of CO2 emission. Even though the slope is 6.8e-9, it indicates a measurable reduction in CO2 emissions. Additionally, the higher \\(R^2\\) and adjusted \\(R^2\\) values signify that this interaction model better fits the data and captures a greater proportion of the variability in emissions compared to the model without the interaction term.\n\n\nCode\n## Fit a linear model with interaction\nmodel_int &lt;- lm(Real_World_CO2_g_mi ~ years + number_of_car + years:number_of_car, data = final_subset)\n\nsumm(model_int, digits = 10)\n\n\n\n\n\n\nObservations\n47\n\n\nDependent variable\nReal_World_CO2_g_mi\n\n\nType\nOLS linear regression\n\n\n\n\n\n\n\n\nF(3,43)\n24.5633246590\n\n\nR²\n0.6315021888\n\n\nAdj. R²\n0.6057930392\n\n\n\n\n\n\n\n\n\nEst.\nS.E.\nt val.\np\n\n\n\n\n(Intercept)\n378.7346753916\n102.8984504157\n3.6806645179\n0.0006442009\n\n\nyears\n-25.7574887204\n6.6647727476\n-3.8647212284\n0.0003704727\n\n\nnumber_of_car\n0.0000002524\n0.0000001292\n1.9534542357\n0.0572918953\n\n\nyears:number_of_car\n0.0000000068\n0.0000000019\n3.5841371809\n0.0008572249\n\n\n\n Standard errors: OLS\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogit regression using Categorical response variable: New and Old Vehicle\nWe will further study the new vehicles have effectively reducing emission. This is the showing two plots, which old vehicle before 2000 year model, and new vehicle after 2000 year model with the linear line.\n\\[\\operatorname{logit}(p)=\\log \\left(\\frac{p}{1-p}\\right)=\\beta_0 + \\beta_1 \\times \\text{car per emission} + \\beta_2 \\times \\text{Model} + \\varepsilon \\]\n\n\nCode\n# Create a new variable 'new_brand' based on the year\n# Filter the dataset to include only \"Car\" and \"Truck\" in Regulatory_Class\nfinal_vehicle_type &lt;- final %&gt;% \n  mutate(new_brand = ifelse(year &gt; 2000, 'new', 'old')) %&gt;%\n  filter(Regulatory_Class %in% c(\"Car\", \"Truck\"))\n\n# Create a jitter plot to show CO2 emissions and new/old classification\nggplot(data = final_vehicle_type, aes(y = new_brand, x = Real_World_CO2_g_mi)) + \n  geom_jitter(width = 0, height = 0.05, alpha = 0.8) +\n  labs(x = \"CO2 Emission per Car\", y = \"Brand New\") +\n  theme_minimal()\n\n\n\n\n\nThis involves showcasing two plots: one for older vehicles predating the year 2000 model and another for newer vehicles following the 2000 model year, each presented with a linear trend line.\n\n\nCode\n# Create a binary variable 'brand_new' based on 'new_brand'\nfinal_vehicle_type &lt;- final_vehicle_type %&gt;%\n  mutate(brand_new = ifelse(new_brand == \"new\", 1, 0))\n\n# Create a jitter plot to visualize CO2 emissions and brand new classification\nbinary &lt;- ggplot(data = final_vehicle_type, aes(y = brand_new, x = Real_World_CO2_g_mi)) + \n  geom_jitter(width = 0, height = 0.05, alpha = 0.8) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"CO2 Emission per Car\", y = \"Brand New\") \nbinary\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe odds ratio of 0.99 indicates that for each one-unit increase in the ‘emission per car’ there is an approximate 1% reduction in emissions for newer vehicle models compared to older ones, tending to exhibit approximately 1% lower emissions concerning their CO2 levels compared to older models. This association highlights the potential for newer vehicle models to contribute slightly less to CO2 emissions in real-world scenarios compared to their older counterparts.\n\n\nCode\n# Create a logistic regression model for predicting 'brand_new'\nmod_new_car &lt;- glm(brand_new ~ Real_World_CO2_g_mi, data = final_vehicle_type, family = 'binomial') \n\n# Create a table summarizing the logistic regression model\nmod_new_car %&gt;% tab_model()\n\n\n\n\n\n \nbrand new\n\n\nPredictors\nOdds Ratios\nCI\np\n\n\n(Intercept)\n260.62\n61.89 – 1258.49\n&lt;0.001\n\n\nReal World CO2 g mi\n0.99\n0.98 – 0.99\n&lt;0.001\n\n\nObservations\n329\n\n\nR2 Tjur\n0.227\n\n\n\n\n\n\n\n\n\nVisualizing logistic regression\nThis is combining two models: linear regression model and logistic regression model, proving insight into both linear and non-linear relationships between old and new vehicles variables.\n\n\nCode\n# Create a binary plot with linear and logistic regression lines\nggplot(data = final_vehicle_type, aes(y = brand_new, x = Real_World_CO2_g_mi)) +\n  geom_jitter(width = 0, height = 0.05, alpha = 0.8) +\n  labs(x = \"CO2 Emission per Car\", y = \"Brand New\") +\n  geom_smooth(method = \"lm\", se = FALSE) +  # Add linear regression line\n  geom_smooth(method = \"glm\", se = FALSE, color = \"red\", \n              method.args = list(family = \"binomial\"))  # Add logistic regression line in red\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nLogistic regression: Truck and Car\nIn the context of environmental impact and vehicle emissions, there’s a growing concern about the differences in CO2 emissions per mile between different types of vehicles. To evaluate and address these concerns, a statistical analysis is conducted to compare the mean CO2 emissions per mile between two categories of vehicles: cars and trucks. This analysis aims to determine whether there’s a significant difference in CO2 emissions per mile between these vehicle types.\n\\[\\operatorname{logit}(p)=\\log \\left(\\frac{p}{1-p}\\right)=\\beta_0 + \\beta_1 \\times \\text{car per emission} + \\beta_2 \\times \\text{Truck} + \\varepsilon \\]\nThe coefficient related to ‘Regulatory_ClassTruck’ in the logistic regression model indicates an interesting finding. It suggests that the odds of a brand new car reducing its emissions are approximately 22 times higher compared to trucks. This translates to an approximate decrease of emissions by 2.8% in a new truck model compared to older models. This suggests that trucks have not significantly improved their efficiency over the span of nearly 50 years.\n\n\nCode\n# Fit the logistic regression model\nmod &lt;- glm(brand_new ~ Real_World_CO2_g_mi + Regulatory_Class, \n           data = final_vehicle_type, family = binomial)\n\n# Extract coefficients, exponentiate, and tidy the results\nexp(coef(mod)) \n\n\n          (Intercept)   Real_World_CO2_g_mi Regulatory_ClassTruck \n        39449.4440170             0.9728793            22.0806972 \n\n\n\nHistogram\nThis diagram clearly shows that the distribution of vehicle emissions is highly right-skewed compared to trucks. This indicates that the Environmental Protection Agency (EPA) has successfully regulated car emissions. However, there is a need for more effort to reduce emissions from trucks.\n\n\nCode\ngghistogram(final_vehicle_type, x = \"Real_World_CO2_g_mi\",\n   add = \"mean\", rug = TRUE,\n   color = \"Regulatory_Class\", fill = \"Regulatory_Class\",\n   palette = c(\"#00AFBB\", \"#E7B800\")) + \n  labs(title = \"Comparison of CO2 Emissions between Car and Truck Groups\")\n\n\n\n\n\nThis finding aligns with EPA’s Clean Truck Plan, unveiled on August 5, 2021. The plan aims to curb CO2 gas emissions and other harmful air pollutants from heavy-duty trucks. It outlines a series of three rulemakings that will come into effect for heavy-duty engines and vehicles starting from the model year 2027.\n\n\n\nHypothesis to comparing two models: Car and Truck\n\\[H_{0}: \\mu_{car} - \\mu_{Truck} = 0\\]\n\\[H_{A}: \\mu_{car} - \\mu_{Truck} \\neq 0\\]\nThe p-value of 2.22e-14 is less than any commonly used significance level (such as 0.05), indicating strong evidence to reject the null hypothesis. Therefore, there is strong evidence to suggest that there is a significant difference in CO2 emissions per mile between the car and truck.\nThere is a 95% confidence that the range between 84.8 and 127.7 includes the true difference in CO2 emissions per mile between cars and trucks.\n\n\nCode\n# in Regulatory_Class, \"All\", \"Car\", and \"Truck\", so only left \"Car\" and \"Truck\" in dataset. \nfinal_class &lt;- final %&gt;% filter(Regulatory_Class %in% c(\"Car\", \"Truck\"))\n\n# Separate data into 'Car' and 'Truck' groups\ncar &lt;- final_class %&gt;% filter(Regulatory_Class == \"Car\")\ntruck &lt;- final_class %&gt;% filter(Regulatory_Class == \"Truck\")\n\n# Conduct a t-test to compare CO2 emissions between 'Car' and 'Truck' groups\nt_test_result &lt;- t.test(truck$Real_World_CO2_g_mi, car$Real_World_CO2_g_mi, conf.level = 0.95)\n# Uncomment the next line to display the summary statistics using tab_model\nt_test_result\n\n\n\n    Welch Two Sample t-test\n\ndata:  truck$Real_World_CO2_g_mi and car$Real_World_CO2_g_mi\nt = 9.7546, df = 283.31, p-value &lt; 0.00000000000000022\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n  84.81797 127.70204\nsample estimates:\nmean of x mean of y \n  520.268   414.008"
  },
  {
    "objectID": "posts/2023-12-9-transport-emission/index.html#conclusion",
    "href": "posts/2023-12-9-transport-emission/index.html#conclusion",
    "title": "Vehicles emission affects to the climate change",
    "section": "Conclusion",
    "text": "Conclusion\nThe analysis highlights the impact of Earth’s surface temperature and regulatory standards, like Corporate Average Fuel Economy (CAFE), on reducing emissions per car. Logistic regression underscores the lower emissions of newer vehicles, emphasizing the need for ongoing technological advancements. The significant disparity in emissions between cars and trucks calls for targeted efforts to enhance truck efficiency.\n\nFuture work\nFuture efforts should prioritize the development of an improved model to enhance both the R-squared value and p-value. Additionally, addressing omitted variable bias is essential for refining this modeling approach. Continuous policy evaluation is crucial, especially in light of initiatives like the Clean Truck Plan. Strengthening public awareness and advocacy efforts will further promote sustainable practices in transportation.\n\n\nReference\nNHTSA. “Corporate Average Fuel Economy” Accessed Dec 15, 2023. https://www.scribbr.com/category/citing-sources/"
  },
  {
    "objectID": "posts/2023-12-14-marine/index.html",
    "href": "posts/2023-12-14-marine/index.html",
    "title": "Mapping the global potential for marine aquaculture",
    "section": "",
    "text": "Repository: https://github.com/khj9759/marine_farming"
  },
  {
    "objectID": "posts/2023-12-14-marine/index.html#background",
    "href": "posts/2023-12-14-marine/index.html#background",
    "title": "Mapping the global potential for marine aquaculture",
    "section": "Background",
    "text": "Background\nMarine farming could be an important source of sustainable protein worldwide, better than land-based meat production with (Gentry et al) globally mapped potential marine farming areas, considering factors like ship traffic and oxygen levels.\nFind the best Exclusive Economic Zones (EEZ) on the West Coast of the US for cultivating various oyster species. Previous research indicates that oysters thrive under specific conditions:\n\nsea surface temperature: 11-30°C\ndepth: 0-70 meters below sea level"
  },
  {
    "objectID": "posts/2023-12-14-marine/index.html#datasets",
    "href": "posts/2023-12-14-marine/index.html#datasets",
    "title": "Mapping the global potential for marine aquaculture",
    "section": "Datasets",
    "text": "Datasets\n\nSea Surface Temperature\nTo characterize the average sea surface temperature in the region, use the yearly average from 2008 to 2012. The data we’re using is initially derived from NOAA’s 5km Daily Global Satellite Sea Surface Temperature Anomaly v3.1.\n\n\nBathymetry\nTo assess the ocean’s depth, we will utilize the following: General Bathymetric Chart of the Oceans (GEBCO).\n\n\nExclusive Economic Zones\nEstablish maritime borders by delineating Exclusive Economic Zones along the west coast of the United States starting from Marineregions.org."
  },
  {
    "objectID": "posts/2023-12-14-marine/index.html#highlight",
    "href": "posts/2023-12-14-marine/index.html#highlight",
    "title": "Mapping the global potential for marine aquaculture",
    "section": "Highlight",
    "text": "Highlight\n\nMerging vector and raster data\nResampling raster data\nRasterizing and Masking raster data\nIdentifying overlay areas using lapp\nConducting map algebra"
  },
  {
    "objectID": "posts/2023-12-14-marine/index.html#workflow",
    "href": "posts/2023-12-14-marine/index.html#workflow",
    "title": "Mapping the global potential for marine aquaculture",
    "section": "Workflow",
    "text": "Workflow\nBelow is an outline of the steps you should consider taking to achieve the assignment tasks.\n\nlibrary(sf)\nlibrary(dplyr)\nlibrary(spData)\nlibrary(here)\nlibrary(raster)\nlibrary(terra)\nlibrary(tmap)\nlibrary(kableExtra)\n\nrm(list = ls())\nhere::i_am(\"index.qmd\")\n\n\nClean data\nTo begin, load essential packages and set the path, preferably utilizing the “here” package. Proceed by reading the West Coast Exclusive Economic Zones shapefile. Next, import sea surface temperature (SST) rasters for the years 2008 to 2012, combining them into a raster stack. Additionally, read the bathymetry raster (depth.tif). Ensure that all data share a consistent coordinate reference system and reproject any datasets that deviate from the specified projection.\n\n# load necessary package \n\n# set the path \nlist.files(here(\"data\"), pattern = \"average *.tif\", full.names = TRUE)\n\ncharacter(0)\n\n# read SST rasters\nsst_2008 &lt;- rast(here( \"data\",\"average_annual_sst_2008.tif\"))\nsst_2009 &lt;- rast(here(\"data\",\"average_annual_sst_2009.tif\"))\nsst_2010 &lt;- rast(here(\"data\",\"average_annual_sst_2010.tif\"))\nsst_2011 &lt;- rast(here(\"data\",\"average_annual_sst_2011.tif\"))\nsst_2012 &lt;- rast(here(\"data\",\"average_annual_sst_2012.tif\"))\n\n# rename the column of SST rasters\nnames(sst_2008) &lt;- c(\"temp_2008\") \nnames(sst_2009) &lt;- c(\"temp_2009\") \nnames(sst_2010) &lt;- c(\"temp_2010\") \nnames(sst_2011) &lt;- c(\"temp_2011\") \nnames(sst_2012) &lt;- c(\"temp_2012\") \n\n# combine SST rasters \nall_sst &lt;- c(sst_2008,\n             sst_2009,\n             sst_2010,\n             sst_2011,\n             sst_2012)\n\n# read West Coast EEZ \nwc &lt;- st_read(here(\"data\", \"wc_regions_clean.shp\"))\n\nReading layer `wc_regions_clean' from data source \n  `/Users/haejinkim/Documents/UCSB/mysite/posts/2023-12-14-marine/data/wc_regions_clean.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -129.1635 ymin: 30.542 xmax: -117.097 ymax: 49.00031\nGeodetic CRS:  WGS 84\n\n# read in bathymetry\ndepth &lt;- rast(\"data/depth.tif\")\n\n# check the crs\n#st_crs(all_sst)  # 9122 \n#st_crs(depth) # 4326 \n#st_crs(wc) # 4326\n\n# reproject using the terra way\nall_sst_reproj &lt;- project(all_sst, wc)\n\n# check the crs\nst_crs(all_sst_reproj) == st_crs(depth) # true\n\n[1] TRUE\n\n\n\n\nProcess data\nTo proceed, we must process the Sea Surface Temperature (SST) and depth data for eventual combination. The SST and depth data possess slight variations in resolution, extents, and positions. Since we aim to maintain the integrity of the underlying depth data, we’ll employ the nearest neighbor approach to resample it and align it with the SST data.\nCompute the average Sea Surface Temperature (SST) for the period 2008-2012. Convert the SST values from Kelvin to Celsius by subtracting 273.15. Adjust the extent of the depth raster to match that of the SST raster. Acknowledge the differing resolutions between the SST and depth data. Resample the depth data using the nearest neighbor approach to align with the resolution of the SST data. Confirm alignment in resolution, extent, and coordinate reference system between the depth and SST datasets. Evaluate the possibility of stacking the rasters for compatibility verification.\n\n# compute mean SST \nmean_sst &lt;- mean(all_sst_reproj)\n\n# convert sst from K to C\nmean_sst_c &lt;- mean_sst - 273.15\n\n# crop depth rast to match the extent of the SST rast\ndepth_cropped &lt;- crop(depth, terra::ext(mean_sst_c)) \n\n# using nearest neighbor approach to resample\ndepth_resampled &lt;- resample(depth_cropped,\n                mean_sst_c,\n                method = \"near\")\n\n# stack to check if they have the same resolution\nresolution_test &lt;- c(depth_resampled,\n                       all_sst_reproj) # no error message means same resolution \n\n\n\nFind suitable locations\nTo find suitable locations for marine aquaculture, reclassify SST and depth data based on oyster suitability. Set values to 1 for suitable locations and NA for unsuitable ones. Identify areas satisfying both SST and depth conditions using the lapp() function to overlay and multiply cell values.\n\n# set suitable values \nrcl_sst &lt;- matrix(c(-Inf, 11, NA, \n                     11, 30, 1, \n                    30, Inf, NA), \n              ncol = 3, byrow = TRUE) #anything outside of range is NA\n\nrcl_depth &lt;- matrix(c(-Inf, -70, NA,\n                      -70, 0, 1,\n                       0, Inf, NA),\n                    ncol = 3, byrow = TRUE)\n\n# reclassifying raster using a reclassification matrix\nsuitable_sst  &lt;- classify(mean_sst_c,\n                     rcl = rcl_sst, \n                     include.lowest = TRUE)\n\nsuitable_depth &lt;- classify(depth_resampled,\n                     rcl = rcl_depth,\n                     include.lowest = TRUE)\n\n# define function\nmult_fun &lt;- function(x, y) {\n  return(x*y)}\n\n# find locations that satisfy both conditions\noverlay_suitable &lt;- lapp(c(suitable_sst, suitable_depth), mult_fun)\n\nplot(overlay_suitable)\n\n\n\n\n\n\nFind the pixel area\nThis is a discovery determined manually using the Earth’s radius.\n\n# figure out one pixel size and overall size \nres(overlay_suitable) # 0.04165905, 0.04165905\n\n[1] 0.04165905 0.04165905\n\ndim(overlay_suitable) # 480 x 480 \n\n[1] 480 408   1\n\next(overlay_suitable) # -131.98475233, -114.987860801091, 29.9920799888132, 49.988422964 (xmin, xmax, ymin, ymax)\n\nSpatExtent : -131.98475233, -114.987860801091, 29.9920799888132, 49.988422964 (xmin, xmax, ymin, ymax)\n\n# Assuming the CRS is in decimal degrees\npixel_size_degrees &lt;- 0.04165905  # Replace this with your actual pixel size in degrees\n\n# Convert degrees to kilometers (approximation)\nlatitude_degrees &lt;- mean(c(29.9920799888132, 49.988422964))  # Mean latitude of the extent\nlongitude_degrees &lt;- mean(c(-131.98475233, -114.987860801091))  # Mean longitude of the extent\n\n# Conversion factors (approximation)\nkm_per_degree_latitude &lt;- 111  # Approximation for latitude (1 degree of latitude is approximately 111 km)\nkm_per_degree_longitude &lt;- 111 * cos(latitude_degrees * pi / 180)  # Approximation for longitude\n\n# Convert pixel size from degrees to kilometers\npixel_size_km_x &lt;- pixel_size_degrees * km_per_degree_longitude\npixel_size_km_y &lt;- pixel_size_degrees * km_per_degree_latitude\npixel_area_km &lt;- pixel_size_km_x *pixel_size_km_y\n\n# Display the converted pixel sizes in kilometers\nprint(paste(\"Pixel Size in Kilometers (X):\", pixel_size_km_x))\n\n[1] \"Pixel Size in Kilometers (X): 3.54281357277252\"\n\nprint(paste(\"Pixel Size in Kilometers (Y):\", pixel_size_km_y))\n\n[1] \"Pixel Size in Kilometers (Y): 4.62415455\"\n\nprint(paste(\"Demension of Pixel Size in Kilometers (X*Y):\", pixel_area_km)) # 16.382517\n\n[1] \"Demension of Pixel Size in Kilometers (X*Y): 16.3825175023378\"\n\n\n\n\nDetermine the most suitable EEZ\nWe aim to assess the overall suitable area within each Exclusive Economic Zone (EEZ) to prioritize zones. To achieve this, identify suitable cells within West Coast EEZs, calculate the area of grid cells, and determine the total suitable area within each EEZ.\nFind the percentage of suitability for each zone, considering rasterizing EEZ data and potentially joining the suitable area by region onto the EEZ vector data.\n\n# making area to grid cells and mask it \nwc_rast &lt;- rasterize(wc, overlay_suitable, \n                           field = \"rgn\", na.rm = TRUE)\n\nwc_mask &lt;- mask(wc_rast,overlay_suitable)\n\n# find area of each suitable zones with zonal() in each of 5 regions of EEZ\n## no need to use zonal \nwc_zonal &lt;- zonal(overlay_suitable, wc_mask, na.rm = TRUE, fun = \"sum\")\ncolnames(wc_zonal)[colnames(wc_zonal) == \"lyr1\"] &lt;- \"count_pixel\"\n\n## use zonal\nwc_zonal_summary &lt;- wc_zonal %&gt;% \n  group_by(rgn) %&gt;% \n  mutate(suitable_area = count_pixel*pixel_area_km)  # pixel size converts to km^2\n\n#join data\nwc_suitable_EEZ &lt;- full_join(wc, wc_zonal_summary , by = \"rgn\") %&gt;% \n  mutate(count_pixel, \n         percentage_suitable = (suitable_area/area_km2 * 100),\n         .before = geometry)\n\nwc_suitable_EEZ %&gt;% kable() %&gt;% kable_minimal()\n\n\n\n\nrgn\nrgn_key\narea_m2\nrgn_id\narea_km2\ncount_pixel\nsuitable_area\npercentage_suitable\ngeometry\n\n\n\n\nOregon\nOR\n179994061293\n1\n179994.06\n71\n1163.1587\n0.6462206\nMULTIPOLYGON (((-123.4318 4...\n\n\nNorthern California\nCA-N\n164378809215\n2\n164378.81\n11\n180.2077\n0.1096295\nMULTIPOLYGON (((-124.2102 4...\n\n\nCentral California\nCA-C\n202738329147\n3\n202738.33\n238\n3899.0392\n1.9231880\nMULTIPOLYGON (((-122.9928 3...\n\n\nSouthern California\nCA-S\n206860777840\n4\n206860.78\n197\n3227.3559\n1.5601585\nMULTIPOLYGON (((-120.6505 3...\n\n\nWashington\nWA\n66898309678\n5\n66898.31\n162\n2653.9678\n3.9671673\nMULTIPOLYGON (((-122.7675 4...\n\n\n\n\n\n\n\n\n\nVisualize results\nThis visual map displays the regions where conditions are suitable for oyster cultivation. The majority of oysters thrive notably well in the southern and central regions of California. Explore these geographical patterns to gain a comprehensive understanding of oyster distribution.\n\n#set to interactive mode\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntmap_last()\n\nWarning in tmap_last(): A map has not been created yet\n\n\nNULL\n\n#map for total suitable area for oysters by region\noysters &lt;- tm_basemap(\"OpenStreetMap.Mapnik\") +\n  tm_shape(wc_suitable_EEZ) +\n  tm_polygons(col = 'suitable_area',\n              palette = 'RdYlBu',\n              alpha = 0.75,\n              border.col = 'black',\n              title = \"Total Suitable Area\") +\n  tm_text(\"rgn\", size = 0.54) +\n  tm_scale_bar(position = c(\"left\", \"right\"))\n \noysters\n\n\n\n\n\n\nThis illustrates the percentage of the suitable area for oyster within each region. Conversely, Washington State exhibits a significant concentration of oyster habitats.\n\n#set to interactive mode\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\ntmap_last()\n\n\n\n\n\n#map for total suitable area for oysters by region\noysters_percent &lt;- tm_basemap(\"OpenStreetMap.Mapnik\") +\n  tm_shape(wc_suitable_EEZ) +\n  tm_polygons(col = 'percentage_suitable',\n              palette = 'RdYlBu',\n              alpha = 0.75,\n              border.col = 'black',\n              title = \"Total Percent of Suitable Area\") +\n  tm_text(\"rgn\", size = 0.54) +\n  tm_scale_bar(position = c(\"left\", \"right\"))\n \noysters_percent"
  },
  {
    "objectID": "posts/2023-12-14-marine/index.html#peruvian-anchoveta",
    "href": "posts/2023-12-14-marine/index.html#peruvian-anchoveta",
    "title": "Mapping the global potential for marine aquaculture",
    "section": "Peruvian Anchoveta",
    "text": "Peruvian Anchoveta\nTo enhance workflow efficiency, focus on the Peruvian Anchoveta, a species thriving within the depth range of -3 to -80 meters and temperatures spanning 13°C to 23°C. It is noteworthy that the primary habitats of Peruvian Anchoveta are located in South California, suggesting a preference for warmer waters at specific depths. Explore these precise environmental preferences to gain profound insights into the ecological distribution of Peruvian Anchoveta.\n\ntest &lt;- function(sst_low, sst_high, depth_low, depth_high, species){\n\n# Classify SST data: Set suitable values to 1 and unsuitable values to NA\nrcl_sst&lt;- matrix(c(-Inf, sst_low, NA, \n                     sst_low, sst_high, 1, \n                    sst_high, Inf, NA), \n              ncol = 3, byrow = TRUE) #anything outside of range is NA\n\nrcl_depth &lt;- matrix(c(-Inf, depth_low, NA,\n                      depth_low, depth_high , 1,\n                       depth_high , Inf, NA),\n                    ncol = 3, byrow = TRUE)\n\n#reclassifying raster using a reclassification matrix\nsuitable_sst &lt;- classify(mean_sst_c,\n                     rcl = rcl_sst, \n                     include.lowest = TRUE)\n\nsuitable_depth &lt;- classify(depth_resampled,\n                     rcl = rcl_depth,\n                     include.lowest = TRUE)\n\n#define function\nmult_fun &lt;- function(x, y) {\n  return(x*y)}\n\n#find locations that satisfy both conditions\noverlay_suitable &lt;- lapp(c(suitable_sst, suitable_depth), mult_fun)\n\n# making area to grid cells and mask it \nwc_rast &lt;- rasterize(wc, overlay_suitable, \n                           field = \"rgn\", na.rm = TRUE)\n\nwc_mask &lt;- mask(wc_rast,overlay_suitable)\n\n# find area of each suitable zones with zonal() in each of 5 regions of EEZ\n## no need to use zonal \nwc_zonal &lt;- zonal(overlay_suitable, wc_mask, na.rm = TRUE, fun = \"sum\")\ncolnames(wc_zonal)[colnames(wc_zonal) == \"lyr1\"] &lt;- \"count_pixel\"\n\n# bring the pixel size \npixel_area_km &lt;- 16.38252\n\n## use zonal\nwc_zonal_summary &lt;- wc_zonal %&gt;% \n  group_by(rgn) %&gt;% \n  mutate(suitable_area = count_pixel*pixel_area_km)\n\n# join the two dataframe \nwc_suitable_EEZ &lt;- full_join(wc, wc_zonal_summary, by = \"rgn\") %&gt;% \n  mutate(percentage_suitable = (suitable_area/area_km2 * 100),\n         .before = geometry) \n\ntmap_mode(\"view\")\ntmap_last()\n\n#map for total suitable area by region\n\narea_map &lt;- tm_basemap(\"OpenStreetMap.Mapnik\") +\n  tm_shape(wc_suitable_EEZ) +\n  tm_polygons(col = 'suitable_area',\n              palette = 'Blues',\n              alpha = 0.75,\n              style = \"jenks\",\n              border.col = 'black',\n              title = paste0(\"Total \" , species, \" Suitable Area\")) +\n  tm_text(\"rgn\", size = 0.54) +\n  tm_scale_bar(position = c(\"left\", \"right\"))\n\npercent_area_map &lt;- tm_basemap(\"OpenStreetMap.Mapnik\") +\n  tm_shape(wc_suitable_EEZ) +\n  tm_polygons(col = 'percentage_suitable',\n              palette = 'Oranges',\n              alpha = 0.75,\n              style = \"jenks\",\n              border.col = 'black',\n              title = paste0(\"Total \",species ,\" percent Suitable Area\")) +\n  tm_text(\"rgn\", size = 0.54) +\n  tm_scale_bar(position = c(\"left\", \"right\"))\n\ntmap_arrange(area_map, percent_area_map)\n\n}\n\ntest(13, 23, -80, -3, \"Peruvian Anchoveta\")\n\ntmap mode set to interactive viewing"
  },
  {
    "objectID": "posts/2023-12-14-marine/index.html#conclusion",
    "href": "posts/2023-12-14-marine/index.html#conclusion",
    "title": "Mapping the global potential for marine aquaculture",
    "section": "Conclusion",
    "text": "Conclusion\nThis study focuses on the analysis of marine organisms beneath the ocean surface. The temperature and depth of the sea significantly impact marine life. The dataset used in this analysis provides valuable information for understanding the sea environment. While our assessments of the suitability of aquaculture are based on the present ocean conditions, it’s essential to acknowledge that the environment is currently experiencing unprecedented changes. Future efforts to evaluate how climate risks may affect aquaculture potential, considering expected shifts in regional ocean temperatures and productivity, will need to improve long-term predictions and offer more nuanced insights into how climate change will impact individual species.(Gentry et al)"
  },
  {
    "objectID": "posts/2023-12-14-marine/index.html#reference",
    "href": "posts/2023-12-14-marine/index.html#reference",
    "title": "Mapping the global potential for marine aquaculture",
    "section": "Reference",
    "text": "Reference\n\nRebecca. R, Mapping the global potential for marine aquaculture, Nature Ecology & Evolution volume1, pages1317–1324 (2017)"
  },
  {
    "objectID": "posts/2024-3-9-emission/index.html",
    "href": "posts/2024-3-9-emission/index.html",
    "title": "Impact of Vehicle Types on GHG Emissions",
    "section": "",
    "text": "Introduction\nThis blog post aims to explore the impact of vehicle emissions on greenhouse gas emissions through visualizations that address three distinct questions using the same dataset sourced from the Environmental Protection Agency (EPA). Throughout this exploration, we will consider 10 key design elements: graphic forms, text, themes, colors, typography, general design, data contextualization, primary message, accessibility, and diversity, equity, and inclusion (DEI).\n\n\nAbout the data\nIn this data, I have included data pertaining to manufacturing companies, vehicle types, regulatory classes, real-world CO2 emissions, production volumes, production volume percentages, and engine types per vehicle segment since 1975. Real-world CO2 emissions indicate the energy efficiency of cars in reducing oil consumption and greenhouse gas emissions. Production volume units start at 1000 (units), and engine types are defined to distinguish between electric and gasoline cars.\nThe Trends database covers all new light-duty vehicles in the United States. According to the CAFE and GHG regulations, these vehicles are divided into two regulatory classes: passenger cars and light trucks. Each class has separate GHG and fuel economy standards. Pickup trucks, vans, and minivans are categorized as light trucks under NHTSA’s regulatory definitions, while sedans, coupes, and wagons are generally classified as cars. Sport utility vehicles (SUVs) can fall into either category, depending on specific vehicle attributes. Please refer to the diagram below for Regulatory Classes and Vehicle Types used in this report.\n\n\n\nQuestions\nbuild three separative visualizations based on the main question:\n1) General Audience: How Have Car Manufacturers Increased Electric Vehicle Production?\n2) Technical Report: How have trends in preferred car types and energy efficiency evolved?\n3) Presentation: Which Vehicle Types Have Car Manufacturer Brands Increased Their Production Volume?\n\n\nSet up and Data Wrangling\n\n\nCode\nknitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = TRUE)\n\n\n\n\nCode\n# library ------\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2) # use the plot \nlibrary(patchwork) # attached two plot\nlibrary(viridis) # color palatte\nlibrary(unikn)  # load unikn package\nlibrary(ggplot2)\nlibrary(ggimage) # add image \nlibrary(gridExtra)\nlibrary(grid)\n\n## data cleaning process -------\n# data from EPA Car emission \nvehicle &lt;- read_csv(\"data/export_data_by_manufactuer.csv\") %&gt;% \n  janitor::clean_names() # clean the name neatly \n\n# change specific data \nvehicle$model_year&lt;- gsub(\"Prelim. 2022\", \"2022\", vehicle$model_year)\n\n# Specify columns to convert to numeric\nnumeric_cols &lt;- c(\"model_year\", \"production_000\", \"production_share\", \"x2_cycle_mpg\", \"real_world_mpg\", \"real_world_mpg_city\", \"real_world_mpg_hwy\", \"real_world_co2_g_mi\", \"real_world_co2_city_g_mi\", \"real_world_co2_hwy_g_mi\", \"weight_lbs\", \"footprint_sq_ft\", \"engine_displacement\", \"horsepower_hp\", \"acceleration_0_60_time_in_seconds\", \"hp_engine_displacement\", \"hp_weight_lbs\", \"ton_mpg_real_world\", \"drivetrain_front\", \"drivetrain_4wd\", \"drivetrain_rear\", \"transmission_manual\", \"transmission_automatic\", \"transmission_lockup\", \"transmission_cvt_hybrid\", \"transmission_other\", \"fuel_delivery_carbureted\", \"fuel_delivery_gasoline_direct_injection_gdi\", \"fuel_delivery_port_fuel_injection\", \"fuel_delivery_throttle_body_injection\", \"fuel_delivery_other\", \"powertrain_diesel\", \"powertrain_electric_vehicle_ev\", \"powertrain_plug_in_hybrid_electric_vehicle_phev\", \"powertrain_fuel_cell_vehicle_fcv\", \"powertrain_other_incl_cng\", \"powertrain_gasoline_hybrid\", \"powertrain_gasoline\", \"turbocharged_engine\", \"stop_start\", \"cylinder_deactivation\", \"multivalve_engine\", \"variable_valve_timing\", \"average_number_of_gears\", \"transmission_cvt_non_hybrid\", \"x4_or_fewer_gears\", \"x5_gears\", \"x6_gears\", \"x7_gears\", \"x8_gears\", \"x9_or_more_gears\")\n\n# Convert specified columns character to numeric\nvehicle[, numeric_cols] &lt;- lapply(vehicle[, numeric_cols], as.numeric)\n\n\n\n\nData Visuzlization\n\nFirst Visualization:\nIn the first visualization, a bar graph is employed to illustrate production volume changes, facilitating easy comparison between years. Titles, captions, and subtitles are included to provide context and guide viewers through the visualization, effectively communicating the main message of significant EV production increase over 10 years. A minimal theme is adopted to maintain focus on the data, with adjustments made to remove unnecessary background elements and declutter the graph. While a single color is chosen for bars, experimenting with palettes is suggested to enhance appeal and accessibility. Typography adjustments ensure clarity and readability, with the title made bold for emphasis. Grid lines, borders, and unnecessary axis elements are removed to declutter the graph and focus attention on the data. Car symbols are added to contextualize the data and reinforce the subject matter of EV production. The title and subtitle effectively center the primary message around significant EV production increase. While accessibility concerns like colorblind-friendly palettes and alt text for images aren’t explicitly addressed, their importance for inclusivity is acknowledged. The focus on electric vehicle production reflects a DEI perspective by considering broader societal impact.\nAdditionally, it’s important to note that the simplicity of the data and the straightforward comparison between years in the visualization may mitigate the necessity for intricate color palettes. Likewise, while the visualization explores electric vehicle production, which inherently carries societal implications, deeper integration of DEI perspectives could enhance its context and implications further.\n\n\nCode\n# -------------------\n#      Option 1-1\n# -------------------\n\n# sorted electric vehicle \nev_production &lt;- vehicle %&gt;% \n  filter(model_year %in% c(seq(2011, 2021)), vehicle_type == \"All\", manufacturer == \"All\") %&gt;%\n  mutate(ev = (powertrain_electric_vehicle_ev + powertrain_plug_in_hybrid_electric_vehicle_phev) * production_000)\n\nmax_counts &lt;- ev_production %&gt;%\n  group_by(model_year) %&gt;%\n  summarize(max_count = max(ev))\n\n\n# plot the graph \nggplot(ev_production, aes(x = factor(model_year), y = ev)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", fill = \"#BA707C\") +\n  geom_text(aes(label = paste0(round(ev),\"k\")), vjust = -0.5, size = 3, color = \"black\") +  # Add rounded value labels\n  labs(x = \"Model Year\", y = \"Count\") +\n  plot_annotation(\n    title = \"Electric Vehicle Production Volume Change (2011 - 2021) in USA\",\n    caption = 'resource: EPA',\n    subtitle = glue::glue(\"The USA has seen a steady increase in EV production over the last 10 years, with 60 times more \\nEVs produced in 2021 as compared to 2011.\"),\n    theme = theme(plot.title = element_text(size = 14, face = \"bold\"))\n  ) +\n  theme_minimal() +\n  theme(strip.placement = \"outside\",  # Move facet labels outside\n        strip.background = element_blank(),  # Remove background\n        panel.background = element_blank(),  # Remove panel background\n        panel.grid.major = element_blank(),  # Remove major grid lines\n        panel.grid.minor = element_blank(),  # Remove minor grid lines\n        panel.border = element_blank(),      # Remove panel border\n        axis.title.y.left = element_blank(), \n        axis.text.y.left = element_blank(), \n        axis.line.y = element_blank(), \n        axis.line = element_line(color = \"black\"),\n        legend.position = \"none\") +\n  geom_image(aes(x = 3, y = 200, image = \"car2.png\"), size = 0.1) +\n  geom_image(aes(x = 8, y = 450, image = \"car2.png\"), size = 0.1) +\n  geom_image(aes(x = 11, y = 700, image = \"car2.png\"), size = 0.1) + \n  # Add a line segment starting from the image to the right\n  geom_segment(aes(x = 1, y = 20, xend = 2.9, yend = 200), color = \"#2AB823\", linetype = \"dashed\", size = 0.1) + \n  geom_segment(aes(x = 3.1, y = 200, xend = 7.9, yend = 450), color = \"#2AB823\", linetype = \"dashed\", size = 0.1) +\n  geom_segment(aes(x = 8.1, y = 450, xend = 10.9, yend = 700), color = \"#2AB823\", linetype = \"dashed\", size = 0.1) +\n  coord_cartesian(clip = \"off\") # Turn off clipping\n\n\n\n\n\n\n\nSecond Visualization:\nIn the second plot, tailored for a technical report, the statistical analysis centers on the energy efficiency improvements of various vehicle types. The vehicles are categorized into five distinct types: sedan/wagon, car SUV, truck SUV, pickup truck, and minivan/van, with distinctions based on regulatory definitions. Notably, all five types showcase record high fuel economy and record low CO2 emissions. The trend away from sedan/wagons towards lower fuel economy vehicles has been observed, partially offsetting some fleetwide benefits.\nProper text elements and non-gridded themes are applied, enhancing the graph’s suitability for inclusion in technical documentation. Redish hues are employed for cars, while greenish tones represent trucks, aligning colors effectively with vehicle types. Typography is made bold for emphasis, ensuring clarity and highlighting key insights. The clear graphic form highlights volume changes, with distinctions between car and truck SUVs indicated by blue dashed lines.\nLabels specifically for car SUVs, pickups, and minivans/vans are included, providing contextualization and centering the primary message beneath the title. Accessibility considerations are integrated with distinct colors for easy differentiation between vehicle types, enhancing inclusivity and ease of interpretation. The graph delves into how market shifts influence fleetwide benefits, offering valuable insights into the dynamics of energy efficiency improvements across different vehicle categories.\n\n\nCode\n# ------------------------------\n#            Option 1-2\n# ------------------------------  \n# bring necessary data All car brand(manufacturer) \nproduction &lt;- vehicle %&gt;% \n  group_by(manufacturer) %&gt;%\n  filter(manufacturer == \"All\") %&gt;%\n  filter(vehicle_type %in% c(\"Sedan/Wagon\", \"Car SUV\", \"Truck SUV\", \"Minivan/Van\", \"Pickup\"))\n\n\n# Normalize the production_share variable to make the sum 100%\nproduction &lt;- production %&gt;%\n  mutate(normalized_production_share = production_share * 100)\n\n## manually add the data ---\n# find the NA value manullay -- first\n#which(is.na(production$normalized_production_share))\n\n# manually add the data ---\n\nproduction$production_share[48] &lt;- 0.114\nproduction$production_share[96] &lt;- 0.022\nproduction$production_share[144] &lt;- 0.161\nproduction$production_share[192] &lt;- 0.257\nproduction$production_share[240] &lt;- 0.447\n\n# make it reorder of vehicle_type\nproduction$vehicle_type &lt;- factor(production$vehicle_type, \n                                  levels = c(\"Sedan/Wagon\", \"Car SUV\", \"Truck SUV\", \"Minivan/Van\", \"Pickup\"))\n\n# color scale \ncolor_bar &lt;- c(\"#F17878\", \"#F1C2C2\", \"#B1B5FF\",\"#ADA3EF\",\"#681CD1\")\n\nlast_truck &lt;- production %&gt;%\n  filter(regulatory_class == \"Truck\") %&gt;%\n  count(model_year, wt = production_share * 100) %&gt;%\n  slice(n())\n\n# portion of 5 different vehicle types  \nvehicle_type_100 &lt;- ggplot(production, aes(x = model_year, y = normalized_production_share, fill = vehicle_type)) +\n  geom_area() + # cover area all over the graph \n  geom_line(data = production %&gt;% \n            filter(regulatory_class == \"Truck\") %&gt;% \n            count(model_year, wt = production_share * 100),  \n          aes(x = model_year, y = n), inherit.aes = FALSE,  \n          color = \"black\", size = 0.5, linetype = \"dashed\") +\n  scale_y_continuous(expand = expansion(add = c(1, 1))) +\n  annotate(geom = \"text\", x = 2000, y = 75, label = \"Sedan/Wagon\", size = 2.5,color = \"white\") +\n  annotate(geom = \"text\", x = 1989, y = 55, label = \"Car SUV\", size = 2.5, color = \"white\") +\n  annotate(geom = \"curve\", x = 1996, xend = 1992, y = 42.5, yend = 55,\n           arrow = arrow(length = unit(0.2, \"cm\"), ends = \"first\"), size = 0.07, color = \"white\") +\n  annotate(geom = \"text\", x = 2005, y = 32.3, label = \"Truck SUV\", size = 2.5) +\n  annotate(geom = \"text\", x = 1995, y = 22.5, label = \"Minivan/Van\", size = 2.5) +\n  annotate(geom = \"text\", x = 1991, y = 7.5, label = \"Pickup\", size = 2.5, color = \"white\") +\n  labs(title = \"Normalized Production Share Over Time\",\n       x = \"Model Year\",\n       y = \"Normalized Production Share (%)\",\n       linetype = \"Vehicle Type\") +\n  scale_fill_manual(values = color_bar) +\n  theme(plot.title = element_text(size = 11), # change title size \n        strip.background = element_blank(),  # Remove background\n        panel.background = element_blank(),  # Remove panel background\n        panel.grid.major = element_blank(),  # Remove major grid lines\n        panel.grid.minor = element_blank(),  # Remove minor grid lines\n        panel.border = element_blank(),      # Remove panel border\n        axis.line = element_line(color = \"black\"),\n        legend.position = \"none\") +\n   geom_segment(aes(x = 2022, y = 0, xend = 2022, yend = 60), color = \"#681CD1\", size = 0.5, arrow = arrow(type = \"closed\", length = unit(0.05, \"inches\"), ends = \"both\")) +\n  geom_segment(aes(x = 2022, y = 60, xend = 2022, yend = 100), color = \"#F17878\", size = 0.5, arrow = arrow(type = \"closed\", length = unit(0.1, \"inches\"), ends = \"both\")) +\n  geom_rect(aes(xmin = 2018, xmax = 2024, ymin = 74, ymax = 82), fill = \"white\", size = 0.5, color = \"#F17878\") +\n  annotate(\"text\", x = 2021, y = 78, label = \"Car\", color = \"#F17878\", size = 3) +\n  geom_rect(aes(xmin = 2018, xmax = 2027, ymin = 21, ymax = 29), fill = \"white\", size = 0.5, color = \"#681CD1\") +\n  annotate(\"text\", x = 2022.5, y = 25, label = \"Truck\", size = 3, color = \"#681CD1\", fontface = \"bold\")\n\n\n\n# prep1: sort the \"all\" vehicle type file using regulatory_class & model_year\nprep1 &lt;- production %&gt;%\n  group_by(regulatory_class, model_year)\n\n# prep2: for the label number of CO2 emission by car/truck\nlabel_data &lt;- prep1 %&gt;%\n  group_by(regulatory_class) %&gt;%\n  dplyr::summarize(first_point = first(real_world_co2_g_mi),\n                   last_point = last(real_world_co2_g_mi),\n                   first_year = first(model_year),\n                   last_year = last(model_year))\n\n# Define equation and statistics for truck\nmodel_t &lt;- lm(real_world_co2_g_mi ~ model_year, data = production %&gt;% filter(regulatory_class == \"Truck\"))\nrsquared_t &lt;- summary(model_t)$r.squared\nslope_t &lt;- coef(model_t)[2] \n\n# Define equation and statistics for car\nmodel_c &lt;- lm(real_world_co2_g_mi ~ model_year, data = production %&gt;% filter(regulatory_class == \"Car\"))\nrsquared_c &lt;- summary(model_c)$r.squared\nslope_c &lt;- coef(model_c)[2]\n\n# Display R² value and slope for Truck\nemission_vehicle &lt;- ggplot(prep1, aes(x = model_year, y = real_world_co2_g_mi)) +\n  geom_point(size = 0.2) +\n  facet_wrap(~ regulatory_class, scales = \"free_y\", ncol = 1) +\n  labs(x = \"Model Year\", y = \"Real World CO2 (g/mi)\", title = \"Energy Efficiency of Vehicle Type\") +\n  geom_smooth(aes(color = regulatory_class),  # Color by regulatory class\n              linetype = \"solid\",              # Set line type to solid\n              size = 1) +                      # Set line size\n  scale_color_manual(values = c(\"Car\" = \"#F17878\", \"Truck\" = \"#681CD1\")) +  # Set custom colors\n  theme(plot.title = element_text(size = 11), # change title size \n    strip.placement = \"outside\",  # Move facet labels outside\n        strip.background = element_blank(),  # Remove background\n        panel.background = element_blank(),  # Remove panel background\n        panel.grid.major = element_blank(),  # Remove major grid lines\n        panel.grid.minor = element_blank(),  # Remove minor grid lines\n        panel.border = element_blank(),      # Remove panel border\n        axis.line = element_line(color = \"black\"),\n        legend.position = \"none\") +\n  geom_text(data = label_data %&gt;% filter(regulatory_class == \"Truck\"), \n            aes(label = paste(\"Truck R² =\", round(rsquared_t, 3), \n                              \"\\nTruck Slope =\", round(slope_t, 3)), \n                x = last_year - 5, y = last_point + 400, hjust = .5, vjust = .5), \n            size = 3, color = \"black\") +\n  geom_text(data = label_data %&gt;% filter(regulatory_class == \"Car\"), \n            aes(label = paste(\"Car R² =\", round(rsquared_c, 3), \n                              \"\\nCar Slope =\", round(slope_c, 3)), \n                x = last_year - 5, y = last_point + 400, hjust = .5, vjust = .5), \n            size = 3, color = \"black\") +\n  geom_text(data = label_data, aes(label = round(first_point), x = first_year, y = first_point), vjust = -0.5, hjust = -0.2, size = 3) +\n  geom_text(data = label_data, aes(label = round(last_point), x = last_year -1, y = last_point), vjust = -3.5, hjust = -0.2, size = 3) \n\n# display two plot at once\n\nvehicle_type_100 + emission_vehicle +\n  plot_annotation(\n    title = \"Overall Vehicle Trend of the Impact of Greenhouse Gas Emission\",\n    caption = 'Resource: EPA',\n    subtitle = \"The normalized production share graph has seen a steady increase in truck vehicles, whereas a decrease in car vehicles \\nover the last 20 years. The energy efficiency of vehicle type graph shows the efforts to reduce greenhouse gas emissions \\nfrom car makers, and the slope of the graph represents the technology's own effort in reducing GHG emissions. It shows \\nthat the slope of cars is steeper than that of trucks.\",\n    theme = theme(plot.title = element_text(face = \"bold\", size = 14),\n                  plot.subtitle = element_text(size = 9))) + \n  coord_cartesian(clip = \"off\") # Turn off clipping\n\n\n\n\n\n\n\nThird Visualization:\nThe third plot targets the general public, focusing on vehicle volume for well-known brands like Toyota, Mercedes, BMW, and Mazda. The graph emphasizes declining production volumes for car brands in the USA, suggesting a preference for trucks over cars. The aim is to convey that despite efforts to reduce GHG emissions, market preferences drive production towards higher-emission vehicles. The graph utilizes a segment format, with text providing additional information. Themes remain unaltered, as grid lines aid in number comparison. Typography is adjusted to match the first plot’s manufacturing brand. Contextualizing data is deemed unnecessary due to brand recognition. The primary message is centered in the subtitle, and accessibility is enhanced by highlighting only specific brands. The data reflects a DEI perspective, ensuring public understanding of the graph’s intention.\n\n\nCode\n# ------------------------------\n#            Option 3 prep.\n# ------------------------------  \n\n# data prep ----\n\n# car filter data ----\ncar_volume &lt;- vehicle %&gt;%\n  filter(regulatory_class != \"All\", \n         manufacturer != \"All\", \n         vehicle_type == \"All Car\",\n         model_year %in% c('1997', '2021'), !is.na(production_000))\n\n# truck filter data ---\ntruck_volume &lt;- vehicle %&gt;%\n  filter(regulatory_class != \"All\", \n         manufacturer != \"All\", \n         vehicle_type == \"All Truck\",\n         model_year %in% c('1997', '2021'), !is.na(production_000))\n\n# tesla is missing data in 1997, so adding manually \nnew_row &lt;- data.frame(\n  model_year = 1997,\n  production_000 = 0,\n  manufacturer = \"Tesla\"\n)\n\n# tesla, hyundai, bmw, subaru, and mercedes are missing data in 1997, so adding manually \nnew_row_t &lt;- data.frame(\n  model_year = c(1997, 1997,1997,1997,1997),\n  production_000 = c(0, 0, 0, 0, 0),\n  manufacturer = c(\"Tesla\", \"Hyundai\", \"BMW\", \"Subaru\", \"Mercedes\")\n)\n\n# car data in 1997..adding manually: tesla \nc1997 &lt;- car_volume %&gt;%\n  select(model_year, production_000, manufacturer) %&gt;%\n  filter(model_year == 1997 & !is.na(production_000))  %&gt;% bind_rows(new_row) %&gt;% \n   mutate_if(is.ordered, .funs = factor, ordered = F) \n\n# car data in 2021 \nc2021 &lt;- car_volume %&gt;%\n  select(model_year, production_000, manufacturer) %&gt;%\n  filter(model_year == 2021 & !is.na(production_000)) %&gt;%\n   mutate_if(is.ordered, .funs = factor, ordered = F) \n\n# truck data in 1997.. adding manually:  tesla, hyundai, bmw, subaru, and mercedes\nt1997 &lt;- truck_volume %&gt;%\n  select(model_year, production_000, manufacturer) %&gt;%\n  filter(model_year == 1997 & !is.na(production_000))  %&gt;% bind_rows(new_row_t) %&gt;% \n   mutate_if(is.ordered, .funs = factor, ordered = F) \n\n# truck data in 2021 \nt2021 &lt;- truck_volume %&gt;%\n  select(model_year, production_000, manufacturer) %&gt;%\n  filter(model_year == 2021 & !is.na(production_000)) %&gt;%\n   mutate_if(is.ordered, .funs = factor, ordered = F) \n\n# car data df \ncar_df &lt;- data.frame(manufacturer = c1997$manufacturer,\n                    start = c1997$production_000,\n                    end = c2021$production_000) \n# add new column \ncar_df$profit &lt;- ifelse(car_df$manufacturer %in% c(\"Toyota\", \"Mercedes\", \"Mazda\", \"BMW\"), 1, 0)\n\n# truck data df \ntruck_df &lt;- data.frame(manufacturer = t1997$manufacturer,\n                    start = t1997$production_000,\n                    end = t2021$production_000) \n\n# sorted only usa brand \ntruck_df$profit &lt;- ifelse(truck_df$manufacturer %in% c(\"Toyota\", \"Mercedes\", \"Mazda\", \"BMW\"), 1, 0)\n\n\nIn the United States, there are 14 different car manufacturer brands with manufacturing plants.\nHere, I explored car and truck production across the major vehicle manufacturer brands from 1997 - 2021. You see those manufacturer brands on the y-axis and production volume on the x-axis for car and truck.\n\n\nCode\n# ------------------------------\n#             Option 3-1\n# ------------------------------  \n\n# Creating a plot with only the y-axis\ntruck_plt &lt;- ggplot(truck_df, aes(x = \"\", y = manufacturer)) +\n  geom_blank() +  # Adding a blank layer to remove any data points\n  theme_minimal() +\n  labs(y = \"Manufacturer brand\", x = NULL) +  # Keep y-axis, remove x-axis label\n  theme(axis.title.x = element_blank(),  # Remove x-axis label\n        axis.text.x = element_blank(),   # Remove x-axis ticks\n        axis.ticks.x = element_blank(),  # Remove x-axis ticks\n        panel.grid.major = element_blank(), # Remove major gridlines\n        panel.grid.minor = element_blank()) # Remove minor gridlines\n\n# show the plot\ntruck_plt + plot_annotation(\n    title = \"Volume Change by Vehicle Types (1997 - 2021) in USA\",\n    caption = 'resource: EPA',\n    subtitle = \"Over the past 14 years, the production of truck vehicles by manufacturers such as Toyota, Mercedes, Mazda, and BMW has shown \\na significant increase, averaging 1200k per brand. In contrast, car production has experienced a decrease.\",\n    theme = theme(plot.title = element_text(size = 14, face = \"bold\"),\n                  plot.subtitle = element_text(size = 9))) +\n  coord_cartesian(clip = \"off\") # Turn off clipping\n\n\n\n\n\nTrucks are the primary source of greenhouse gas emissions compared to cars. By examining the manufacturing data of car companies, we can identify the brands that are the major contributors to greenhouse gas emissions. Focusing solely on truck production, we observe that Toyota, Mercedes, Mazda, and BMW have consistently produced the highest volumes, each exceeding 500k to 1500k units from 1997 to 2021.\n\n\nCode\n# truck plot for geom_segment\ntruck_plt &lt;- ggplot(truck_df , aes(x = start, xend = end, \n                   y = fct_reorder(manufacturer, manufacturer), yend = manufacturer)) + \n  geom_segment(aes(position = \"stack\", color = factor(profit)), arrow = arrow(type = \"closed\", length = unit(0.1, \"inches\")), show.legend = FALSE) +\n  geom_point(aes(x = start, y = manufacturer), color = \"#CCD437\", size = 1.5) +\n  scale_color_manual(values = c(\"0\" = \"grey\", \"1\" = \"black\"),\n                     labels = c(\"Start\", \"End\", \"Segment\")) +\n  labs(x = \"Production Volume (K)\", title = \"Truck\", y = \"Manufacturer Brand\") +\n  # add box to highlighted one \n  geom_rect(aes(xmin = 0, xmax = 3000, ymin = 0.5, ymax = 1.5), fill = \"transparent\", size = 0.5, color = \"#F17878\") +\n  geom_rect(aes(xmin = 0, xmax = 3000, ymin = 6.5, ymax = 8.5), fill = \"transparent\", size = 0.5, color = \"#F17878\") +\n  geom_rect(aes(xmin = 0, xmax = 3000, ymin = 12.5, ymax = 13.5), fill = \"transparent\", size = 0.5, color = \"#F17878\") +\n  theme_minimal() +\n  theme()  # Remove title\n\n# show the plot\ntruck_plt + plot_layout(widths = c(1, 1)) + \n  plot_annotation(\n    title = \"Volume Change by Vehicle Types (1997 - 2021) in USA\",\n    caption = 'resource: EPA',\n    subtitle = \"Over the past 14 years, the production of truck vehicles by manufacturers such as Toyota, Mercedes, Mazda, and BMW has shown \\na significant increase, averaging 1200k per brand. In contrast, car production has experienced a decrease.\",\n    theme = theme(plot.title = element_text(size = 14, face = \"bold\"),\n                  plot.subtitle = element_text(size = 9))) +\n  coord_cartesian(clip = \"off\") # Turn off clipping\n\n\n\n\n\nDespite efforts by the EPA to reduce greenhouse gas emissions and regulate car emissions, trucks still maintain high energy efficiency, contributing significantly to GHG emissions. Particularly, Toyota, Mercedes, Mazda, and BMW have shown a trend of increasing truck production while simultaneously minimizing car production through concerted efforts. Consequently, these four brands have made substantial contributions to greenhouse gas emissions compared to others.\n\n\nCode\n# car plot for geom_segment\ncar_plt &lt;- ggplot(car_df, aes(x = start, xend = end, \n                   y = fct_reorder(manufacturer, manufacturer), yend = manufacturer)) + \n  geom_segment(aes(position = \"stack\", color = factor(profit)), arrow = arrow(type = \"closed\", length = unit(0.1, \"inches\")), show.legend = FALSE) +\n  geom_point(aes(x = start, y = manufacturer), color = \"#CCD437\", size = 1.5) +\n  scale_color_manual(values = c(\"0\" = \"grey\", \"1\" = \"black\"),\n                     labels = c(\"Start\", \"End\", \"Segment\")) +\n  labs(x = \"Production Volume (K)\", title = \"Car\", y = NULL) +  # Set a general title\n  theme_minimal() +\n  theme(axis.title.y = element_blank()) +\n    # add box to highlighted one \n  geom_rect(aes(xmin = 0, xmax = 3000, ymin = 0.5, ymax = 1.5), fill = \"transparent\", size = 0.5, color = \"#F17878\") +\n  geom_rect(aes(xmin = 0, xmax = 3000, ymin = 6.5, ymax = 8.5), fill = \"transparent\", size = 0.5, color = \"#F17878\") +\n  geom_rect(aes(xmin = 0, xmax = 3000, ymin = 12.5, ymax = 13.5), fill = \"transparent\", size = 0.5, color = \"#F17878\") +\n  theme_minimal() +\n  theme(axis.title.y.left = element_blank(), \n        axis.text.y.left = element_blank(),\n        axis.line.y = element_blank())  \n\n# truck ------\ntruck_plt &lt;- ggplot(truck_df , aes(x = start, xend = end, \n                   y = fct_reorder(manufacturer, manufacturer), yend = manufacturer)) + \n  geom_segment(aes(position = \"stack\", color = factor(profit)), arrow = arrow(type = \"closed\", length = unit(0.1, \"inches\")), show.legend = FALSE) +\n  geom_point(aes(x = start, y = manufacturer), color = \"#CCD437\", size = 1.5) +\n  scale_color_manual(values = c(\"0\" = \"grey\", \"1\" = \"black\"),\n                     labels = c(\"Start\", \"End\", \"Segment\")) +\n  labs(x = \"Production Volume (K)\", title = \"Truck\", y = \"Manufacturer Brand\") +\n  # add box to highlighted one \n  geom_rect(aes(xmin = 0, xmax = 3000, ymin = 0.5, ymax = 1.5), fill = \"transparent\", size = 0.5, color = \"#F17878\") +\n  geom_rect(aes(xmin = 0, xmax = 3000, ymin = 6.5, ymax = 8.5), fill = \"transparent\", size = 0.5, color = \"#F17878\") +\n  geom_rect(aes(xmin = 0, xmax = 3000, ymin = 12.5, ymax = 13.5), fill = \"transparent\", size = 0.5, color = \"#F17878\") +\n  theme_minimal() +\n  theme()  # Remove title\n\n# Combine plots with shared title\ntruck_plt + car_plt + \n  plot_annotation(\n    title = \"Volume Change by Vehicle Types (1997 - 2021) in USA\",\n    caption = 'resource: EPA',\n    subtitle = \"Over the past 14 years, the production of truck vehicles by manufacturers such as Toyota, Mercedes, Mazda, and BMW has shown \\na significant increase, averaging 1200k per brand. In contrast, car production has experienced a decrease.\",\n    theme = theme(plot.title = element_text(size = 14, face = \"bold\"),\n                  plot.subtitle = element_text(size = 9))) +\n  coord_cartesian(clip = \"off\") # Turn off clipping"
  },
  {
    "objectID": "posts/2023-12-09-thomas-fire/thomas_fire_air.html",
    "href": "posts/2023-12-09-thomas-fire/thomas_fire_air.html",
    "title": "Air Quality Analysis of Thomas Fire",
    "section": "",
    "text": "Author: Haejin Kim\nRepository: https://github.com/khj9759/air_quality\n\n\nThis project aims to generate a comprehensive analysis by creating a false color image that vividly portrays the fire scar resulting from the 2017 Thomas Fire. To achieve this, the project utilizes Air Quality Index (AQI) data sourced from the US Environmental Protection Agency. The primary objective is to visually depict the substantial impact of the Thomas Fire on air quality conditions specifically within Santa Barbara County.\n\n\n\n\nEmployed Pandas for data manipulation and time-series analysis\nGeoPandas for geospatial tasks including patches and clipping\nCreated clear map legends using Matplotlib’s mpatches\nRioxarray for efficient raster data handling\n\n\n\n\n\n\nThis dataset comprises simplified bands (red, green, blue, near-infrared, and shortwave infrared) derived from the Landsat Collection 2 Level-2 atmospherically corrected surface reflectance data, collected by the Landsat 8 satellite. The data underwent preprocessing in the Microsoft Planetary Computer, involving the removal of non-land data and spatial resolution reduction (Landsat Collection in MPC). It is intended for visualization purposes only.\n\n\n\nA shapefile detailing fire perimeters in California during 2017. The complete file is accessible via the CA state geoportal.\n\nfire_name: Name of Fire\ngeometry\n\nData resource: https://gis.data.ca.gov/datasets/CALFIRE-Forestry::california-fire-perimeters-all-1/about\n\n\n\nThis dataset was obtained from the EPA data portal. It contains 10 columns with information about statename, countyname, AQI, Defining Parameter, and Date. For the purpose of this analysis, the following columns will be utilized:\n\nAQI: Air Quality Index\nDate: time to measure\nCounty Name\nNumber of Report: Report to Fire\nDefining Parameter: Method to measure air quality\n\nData resource: https://www.epa.gov/\n\n\n\n\nThis document aims to produce two main outputs:\n\nCreate a graph of the Daily AQI and 5-Day Average\n\n\n\nMapping the Thomas Fire area in Santa Barbara\n\n\n\n\n\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches # for creating legends\n\nimport rioxarray as rioxr\nimport geopandas as gpd\n\nfrom rasterio.features import rasterize # for rasterizing polygons\n\n\npd.set_option('display.max_columns', None)"
  },
  {
    "objectID": "posts/2023-12-09-thomas-fire/thomas_fire_air.html#about",
    "href": "posts/2023-12-09-thomas-fire/thomas_fire_air.html#about",
    "title": "Air Quality Analysis of Thomas Fire",
    "section": "",
    "text": "This project aims to generate a comprehensive analysis by creating a false color image that vividly portrays the fire scar resulting from the 2017 Thomas Fire. To achieve this, the project utilizes Air Quality Index (AQI) data sourced from the US Environmental Protection Agency. The primary objective is to visually depict the substantial impact of the Thomas Fire on air quality conditions specifically within Santa Barbara County."
  },
  {
    "objectID": "posts/2023-12-09-thomas-fire/thomas_fire_air.html#highlights",
    "href": "posts/2023-12-09-thomas-fire/thomas_fire_air.html#highlights",
    "title": "Air Quality Analysis of Thomas Fire",
    "section": "",
    "text": "Employed Pandas for data manipulation and time-series analysis\nGeoPandas for geospatial tasks including patches and clipping\nCreated clear map legends using Matplotlib’s mpatches\nRioxarray for efficient raster data handling"
  },
  {
    "objectID": "posts/2023-12-09-thomas-fire/thomas_fire_air.html#about-the-data",
    "href": "posts/2023-12-09-thomas-fire/thomas_fire_air.html#about-the-data",
    "title": "Air Quality Analysis of Thomas Fire",
    "section": "",
    "text": "This dataset comprises simplified bands (red, green, blue, near-infrared, and shortwave infrared) derived from the Landsat Collection 2 Level-2 atmospherically corrected surface reflectance data, collected by the Landsat 8 satellite. The data underwent preprocessing in the Microsoft Planetary Computer, involving the removal of non-land data and spatial resolution reduction (Landsat Collection in MPC). It is intended for visualization purposes only.\n\n\n\nA shapefile detailing fire perimeters in California during 2017. The complete file is accessible via the CA state geoportal.\n\nfire_name: Name of Fire\ngeometry\n\nData resource: https://gis.data.ca.gov/datasets/CALFIRE-Forestry::california-fire-perimeters-all-1/about\n\n\n\nThis dataset was obtained from the EPA data portal. It contains 10 columns with information about statename, countyname, AQI, Defining Parameter, and Date. For the purpose of this analysis, the following columns will be utilized:\n\nAQI: Air Quality Index\nDate: time to measure\nCounty Name\nNumber of Report: Report to Fire\nDefining Parameter: Method to measure air quality\n\nData resource: https://www.epa.gov/"
  },
  {
    "objectID": "posts/2023-12-09-thomas-fire/thomas_fire_air.html#final-output",
    "href": "posts/2023-12-09-thomas-fire/thomas_fire_air.html#final-output",
    "title": "Air Quality Analysis of Thomas Fire",
    "section": "",
    "text": "This document aims to produce two main outputs:\n\nCreate a graph of the Daily AQI and 5-Day Average\n\n\n\nMapping the Thomas Fire area in Santa Barbara"
  },
  {
    "objectID": "posts/2023-12-09-thomas-fire/thomas_fire_air.html#import-libraries",
    "href": "posts/2023-12-09-thomas-fire/thomas_fire_air.html#import-libraries",
    "title": "Air Quality Analysis of Thomas Fire",
    "section": "",
    "text": "import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches # for creating legends\n\nimport rioxarray as rioxr\nimport geopandas as gpd\n\nfrom rasterio.features import rasterize # for rasterizing polygons\n\n\npd.set_option('display.max_columns', None)"
  },
  {
    "objectID": "posts/2023-12-09-thomas-fire/thomas_fire_air.html#data-cleaning",
    "href": "posts/2023-12-09-thomas-fire/thomas_fire_air.html#data-cleaning",
    "title": "Air Quality Analysis of Thomas Fire",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nConvert the column names to lowercase and replace ’ ’ with ’_’ for both ca_fire and aqi.\n\n# make column names small caps\nca_fire.columns = ca_fire.columns.str.lower()\nprint('column name of ca_fire: ',ca_fire.columns, \"\\n\")\n\n# re-assign the column names - .str.lower() makes them lower case\n#  re-assign the column names again - .str.replace(' ','_') replaces the space for _\naqi.columns = aqi.columns.str.lower()\naqi.columns = aqi.columns.str.replace(' ','_')\nprint('column name of aqi: ',aqi.columns, '\\n')\n\ncolumn name of ca_fire:  Index(['index', 'objectid', 'year_', 'state', 'agency', 'unit_id', 'fire_name',\n       'inc_num', 'alarm_date', 'cont_date', 'cause', 'c_method', 'objective',\n       'gis_acres', 'comments', 'complex_na', 'complex_in', 'irwinid',\n       'fire_num', 'decades', 'shape_leng', 'shape_area', 'geometry'],\n      dtype='object') \n\ncolumn name of aqi:  Index(['state_name', 'county_name', 'state_code', 'county_code', 'date', 'aqi',\n       'category', 'defining_parameter', 'defining_site',\n       'number_of_sites_reporting'],\n      dtype='object') \n\n\n\nFor the AQI data, narrow down the scope to focus on California and Santa Barbara. Remove unnecessary columns such as state_name, county_name, state_code, and county_code.\n\n# new data frame 'aqi_sb' select only data California and Santa Barbara. \naqi_sb = aqi[(aqi.state_name == 'California') & (aqi.county_name == 'Santa Barbara')].copy()\n\n# remove `state_name`, `county_name`, `state_code` and `county_code` columns\naqi_sb = aqi_sb.drop(columns = ['state_name','county_name','state_code','county_code'])\n\naqi_sb.dtypes\n\ndate                         object\naqi                           int64\ncategory                     object\ndefining_parameter           object\ndefining_site                object\nnumber_of_sites_reporting     int64\ndtype: object\n\n\nAs observed, the ‘aqi_sb’ column appears to contain objects. To convert these objects into datetime values, you can utilize the ‘precip.DATE’ column and then apply the ‘set_index’ function.\n\n# covert precip.DATE column to timedate objects\naqi_sb.date = pd.to_datetime(aqi_sb.date)\naqi_sb = aqi_sb.set_index('date')\n\nIndexing the DataFrame with a DateTime index.\n\n# Check the updated index of aqi_sb and its data type\nindex_dtype = aqi_sb.index\nprint(index_dtype)\n\nDatetimeIndex(['2017-01-01', '2017-01-02', '2017-01-03', '2017-01-04',\n               '2017-01-05', '2017-01-06', '2017-01-07', '2017-01-08',\n               '2017-01-09', '2017-01-10',\n               ...\n               '2018-12-22', '2018-12-23', '2018-12-24', '2018-12-25',\n               '2018-12-26', '2018-12-27', '2018-12-28', '2018-12-29',\n               '2018-12-30', '2018-12-31'],\n              dtype='datetime64[ns]', name='date', length=730, freq=None)"
  },
  {
    "objectID": "posts/2023-12-09-thomas-fire/thomas_fire_air.html#rolling-window-calculations",
    "href": "posts/2023-12-09-thomas-fire/thomas_fire_air.html#rolling-window-calculations",
    "title": "Air Quality Analysis of Thomas Fire",
    "section": "Rolling Window Calculations",
    "text": "Rolling Window Calculations\nUtilize the rolling() method available for pandas.Series to perform rolling window calculations. Setting the parameter to ‘5D’ specifies a window of 5 days, while the aggregator function mean() computes the mean over each window.\n\n# we get a pd.Series as ouput\naqi_sb.aqi.rolling('5D').mean()\n\n# Calculate the 5-day rolling mean of AQI and add it as a new column\naqi_sb['five_day_average']=aqi_sb.aqi.rolling('5D').mean()\n\n# Display the DataFrame with the new 'five_day_average' column\nprint(aqi_sb)\n\n            aqi  category defining_parameter defining_site  \\\ndate                                                         \n2017-01-01   39      Good              Ozone   06-083-4003   \n2017-01-02   36      Good              Ozone   06-083-4003   \n2017-01-03   71  Moderate               PM10   06-083-4003   \n2017-01-04   34      Good              Ozone   06-083-4003   \n2017-01-05   37      Good              Ozone   06-083-4003   \n...         ...       ...                ...           ...   \n2018-12-27   37      Good              Ozone   06-083-1025   \n2018-12-28   39      Good              Ozone   06-083-1021   \n2018-12-29   39      Good              Ozone   06-083-1021   \n2018-12-30   39      Good              Ozone   06-083-1021   \n2018-12-31   38      Good              Ozone   06-083-1021   \n\n            number_of_sites_reporting  five_day_average  \ndate                                                     \n2017-01-01                         12         39.000000  \n2017-01-02                         11         37.500000  \n2017-01-03                         12         48.666667  \n2017-01-04                         13         45.000000  \n2017-01-05                         12         43.400000  \n...                               ...               ...  \n2018-12-27                         11         38.600000  \n2018-12-28                         12         38.600000  \n2018-12-29                         12         38.200000  \n2018-12-30                         12         37.800000  \n2018-12-31                         12         38.400000  \n\n[730 rows x 6 columns]"
  },
  {
    "objectID": "posts/2023-12-09-thomas-fire/thomas_fire_air.html#update",
    "href": "posts/2023-12-09-thomas-fire/thomas_fire_air.html#update",
    "title": "Air Quality Analysis of Thomas Fire",
    "section": "Update",
    "text": "Update\nUtilize the squeeze() function followed by drop() to remove the specified band.\n\n# original dimensions and coordinates\nprint(landsat.dims, landsat.coords,'\\n')\n\n# remove length 1 dimension (band)\nlandsat = landsat.squeeze()\nprint(landsat.dims, landsat.coords,'\\n')\n\n# remove coordinates associated to band\nlandsat = landsat.drop('band')\nprint(landsat.dims, landsat.coords)\n\nFrozen({'band': 1, 'x': 870, 'y': 731}) Coordinates:\n  * band         (band) int64 1\n  * x            (x) float64 1.213e+05 1.216e+05 ... 3.557e+05 3.559e+05\n  * y            (y) float64 3.952e+06 3.952e+06 ... 3.756e+06 3.755e+06\n    spatial_ref  int64 0 \n\nFrozen({'x': 870, 'y': 731}) Coordinates:\n    band         int64 1\n  * x            (x) float64 1.213e+05 1.216e+05 ... 3.557e+05 3.559e+05\n  * y            (y) float64 3.952e+06 3.952e+06 ... 3.756e+06 3.755e+06\n    spatial_ref  int64 0 \n\nFrozen({'x': 870, 'y': 731}) Coordinates:\n  * x            (x) float64 1.213e+05 1.216e+05 ... 3.557e+05 3.559e+05\n  * y            (y) float64 3.952e+06 3.952e+06 ... 3.756e+06 3.755e+06\n    spatial_ref  int64 0"
  },
  {
    "objectID": "posts/2023-12-09-thomas-fire/thomas_fire_air.html#matching-coordinate-reference-systems-crs",
    "href": "posts/2023-12-09-thomas-fire/thomas_fire_air.html#matching-coordinate-reference-systems-crs",
    "title": "Air Quality Analysis of Thomas Fire",
    "section": "Matching Coordinate Reference Systems (CRS)",
    "text": "Matching Coordinate Reference Systems (CRS)\nIn order to generate the map, compare the CRS (Coordinate Reference Systems) of ca_fire and landsat, and ensure they have the same CRS by matching them.\n\n# check CRS\nprint(f\"california_fire: {ca_fire.crs} \\nland sat: {landsat.rio.crs}\\n\")\n\n# transform fishing_areas CRS to epsg:32611\nca_fire = ca_fire.to_crs(landsat.rio.crs)\n\nprint('match crs landsat and ca_fire?:', landsat.rio.crs == ca_fire.crs)\n\ncalifornia_fire: EPSG:3857 \nland sat: EPSG:32611\n\nmatch crs landsat and ca_fire?: True"
  },
  {
    "objectID": "posts/2023-12-09-thomas-fire/thomas_fire_air.html#dissolve-and-clipping",
    "href": "posts/2023-12-09-thomas-fire/thomas_fire_air.html#dissolve-and-clipping",
    "title": "Air Quality Analysis of Thomas Fire",
    "section": "Dissolve and Clipping",
    "text": "Dissolve and Clipping\nAggregate the geometries based on the values of a column such as geometry and fire_name, assigning the resulting structure the name fire_districts.\nClip the landsat data to include only the areas within the boundaries of the fire_districts.\n\n# dissolve by district\nfire_districts = ca_fire[['geometry','fire_name']].dissolve(by='fire_name', as_index=False)\nfire_districts\n\n# clipping the ca perimeter only fire \nfire_districts_clip = landsat.rio.clip_box(*fire_districts.total_bounds)"
  },
  {
    "objectID": "posts/2023-12-09-thomas-fire/thomas_fire_air.html#clipping-to-the-area-of-thomas-fire-exclusively",
    "href": "posts/2023-12-09-thomas-fire/thomas_fire_air.html#clipping-to-the-area-of-thomas-fire-exclusively",
    "title": "Air Quality Analysis of Thomas Fire",
    "section": "Clipping to the Area of Thomas Fire Exclusively",
    "text": "Clipping to the Area of Thomas Fire Exclusively\nClip the dataset to encompass solely the geographical extent affected by the Thomas Fire.\n\n# Registered areas:\n# print(ca_fire.fire_name.unique())\n\n# Filter the 'fire_districts' dataset to obtain the area specifically related to the Thomas Fire\nfire_thomas = fire_districts[fire_districts.fire_name == 'THOMAS']"
  },
  {
    "objectID": "posts/2023-12-09-thomas-fire/thomas_fire_air.html#plotting-daily-aqi-and-5-day-average",
    "href": "posts/2023-12-09-thomas-fire/thomas_fire_air.html#plotting-daily-aqi-and-5-day-average",
    "title": "Air Quality Analysis of Thomas Fire",
    "section": "Plotting Daily AQI and 5-Day Average",
    "text": "Plotting Daily AQI and 5-Day Average\nGenerate a line plot illustrating both the daily Air Quality Index (AQI) and the 5-day average. Overlay the 5-day average on the daily AQI data. Notably, December 2017 recorded substantially high AQI levels in Santa Barbara, CA. This observed surge in AQI is directly linked to the impact of the Thomas fire.\n\n# initialize empty figure\nfig, ax = plt.subplots()\n\n# ---------------------------\n# Create a line plot for daily AQI on the existing ax\naqi_sb.plot(y='aqi', ax=ax, label='Daily AQI', color='grey')\n\n# Create a line plot for the 5-day average on the same ax\naqi_sb.plot(y='five_day_average', ax=ax, label='5-Day Average', color='brown')\n\n\n# ----------------------------\n# Set plot title and labels\nax.set_title('5-Day Average on Top of Daily AQI for Santa Barbara, California')\nax.set_xlabel('Date')\nax.set_ylabel('AQI Value')\n\n# display figure\nplt.show()\n#plt.savefig('image/AQI_SB.png', bbox_inches='tight',  dpi=100)"
  },
  {
    "objectID": "posts/2023-12-09-thomas-fire/thomas_fire_air.html#map",
    "href": "posts/2023-12-09-thomas-fire/thomas_fire_air.html#map",
    "title": "Air Quality Analysis of Thomas Fire",
    "section": "Map",
    "text": "Map\nDisplays the false color image composed of shortwave infrared (SWIR), near-infrared (NIR), and red bands. Additionally, overlay the perimeter of the Thomas fire on this map.\n\n\nfig, ax = plt.subplots()\n\n# ---------------------------------\n# bring the clipping map\nfire_districts_clip[['swir22','nir08','red']].to_array().plot.imshow(robust = True)\n\n\n# bring the fire thomas map\nfire_thomas.plot(ax=ax, edgecolor='red', color='none')\n\n# ---------------------------------\n# create the patch\nfire_thomas_patch = mpatches.Patch(edgecolor='red', facecolor='none', linewidth=3,\n                              label='Thomas fire')\n \n# ---------------------------------\n# Adding legend with custom label colors and settings\nlegend = ax.legend(handles=[fire_thomas_patch], frameon=False,loc='upper left', bbox_to_anchor= (1, 1))\nax.set_title('Thomas Fire area in the map')\n\nplt.show()  \n#plt.savefig('image/Thomas_fire_map.png', bbox_inches='tight',  dpi=100)"
  },
  {
    "objectID": "posts/2022-12-08-QGIS/index.html",
    "href": "posts/2022-12-08-QGIS/index.html",
    "title": "Earthquake",
    "section": "",
    "text": "California located the most active faults on the Ring of Fire. San Andreas fault line, the strongest line near the Ring of Fire, penetrating the California area, stretches along the central west coast of North America. My hypothesis model is based on the measure of the population near the fault lines in San Francisco Bay Area. I assume that near the fault lines have a high potential risk of earthquake. Comparing the actual model, which was published by California Geological Survey, contains the actual geological data and geographic features. The population of my hypothesis model is 1.5 times less than the actual model. Moreover, my hypothesis model shows no passing through any fault line in San Francisco city, but the actual model defines San Francisco city has a potential risk from earthquakes, which means there are no safe places in San Francisco Bay Area."
  },
  {
    "objectID": "posts/2022-12-08-QGIS/index.html#abstract",
    "href": "posts/2022-12-08-QGIS/index.html#abstract",
    "title": "Earthquake",
    "section": "",
    "text": "California located the most active faults on the Ring of Fire. San Andreas fault line, the strongest line near the Ring of Fire, penetrating the California area, stretches along the central west coast of North America. My hypothesis model is based on the measure of the population near the fault lines in San Francisco Bay Area. I assume that near the fault lines have a high potential risk of earthquake. Comparing the actual model, which was published by California Geological Survey, contains the actual geological data and geographic features. The population of my hypothesis model is 1.5 times less than the actual model. Moreover, my hypothesis model shows no passing through any fault line in San Francisco city, but the actual model defines San Francisco city has a potential risk from earthquakes, which means there are no safe places in San Francisco Bay Area."
  },
  {
    "objectID": "posts/2022-12-08-QGIS/index.html#introduction",
    "href": "posts/2022-12-08-QGIS/index.html#introduction",
    "title": "Earthquake",
    "section": "Introduction",
    "text": "Introduction\nCalifornia generally has two- or three-times strong earthquakes enough to cause moderate damage each year. California is located especially near the Ring of Fire. In history, the recent rapture that happened in 1906, caused the great California earthquake. Despite a quick response from San Francisco’s large military population, the city was devasted. The earthquake killed 3,000 people and created 400,000 residents homeless. (National Archives)\nThis major event happened in San Andreas. San Andreas fault lines pass through the middle of California, which primary plate boundary structure in California and present a large earthquake region. Meanwhile, the San Andreas fault is located in the space where the North American and the Pacific plate meet. (David K. Lynch, 2006) The San Andreas fault is the primary plate boundary fault in southern California, and it produces the most major earthquakes. Therefore, this study is mainly focused on people have affected by earthquake in San Francisco Bay Area. This report also investigates people living near the San Andreas fault lines.\nThis study is mainly focused on people have affected by earthquake in San Francisco Bay Area. My hypothesis model is assuming people near the fault lines having affect the earthquake shaking. The actual model derived the data from USGS, and it includes the actual geological information. Ultimately, my hypothesis model and actual model compare the population affected by the earthquake."
  },
  {
    "objectID": "posts/2022-12-08-QGIS/index.html#methods",
    "href": "posts/2022-12-08-QGIS/index.html#methods",
    "title": "Earthquake",
    "section": "Methods",
    "text": "Methods\n\nFigure 1: Ring of Fire. The ring of active volcanoes and earthquakes frames the Pacific Ocean- about 90% of the world’s earthquakes occur here. California is also closely located in this Ring of Fire.\nCalifornia is the most vulnerable State to an earthquake in the United States. Figure 1 shows that the Ring of Fire is closely located in the California region. This active belt of earthquake epicenters, volcanoes and tectonic plate boundaries fringe the Pacific basin. Most of the world’s largest earthquakes occur in the Ring of Fire, and 90 % of earthquake happens in this area. (USGS, 2012) Most of the strongest Seismic fault lines are located in California, close to the Ring of Fire. Among the fault lines in California, San Andreas occupies the most energy and rupture in large earthquakes. (Hardy, 2020)\n\nFigure 2: Seismic Fault lines and Population density in San Francisco Bay Area. 81 different Seismic fault lines of San Francisco Bay Area.\nFault lines intertwined all of the San Francisco Bay areas. Regardless of the fault lines, the population density shows a quite higher level near the fault lines. Most fault lines, which indicate brown lines, pass over all of the San Francisco Bay areas, except the city of San Francisco. Fault lines intertwine in Santa Clara with high density. The San Andreas, the blue lines, pass through Marin, San Mateo, and Santa Clara counties.\nThis study mainly focused on those people living near all fault lines and compare the people living near the fault lines. San Francisco Bay Area is the second-highest population in the United States. (San Francisco Chronicle, 2021) Due to the high population, a large portion of people suffers earthquake near the fault lines, especially San Francisco Bay Area. This area contains 9 different counties, which are San Francisco, Sonoma, Napa, Marin, Solano, Contra Costa, San Mateo, Alameda, and Santa Clara. In Figure 2, it shows the clear the 9 different counties with 81 fault lines.\nThe dataset consists of the United States Geological Survey (USGS) for fault lines, and this dataset includes the 539 different fault lines in California and among those, the 81 fault lines in San Francisco Bay Area. The actual model of earthquake shaking potential risk data brought Dr. Branum’s 2015 model from USGS, and Dr. Branum organized to develop this model under California Geological Survey and United State Geological Survey.\nThe recent census data is the most difficult to find because recent one shows the counties based, not the small towns. That form of data structure was not proper to use my hypothesis model. Helping from Kaitlyn Bretz, who is TA in this course, she tried to find proper dataset, but eventually the population data used from the 2010 census in this research.\nParticularly, this report used the QGIS program, which helps to define the project map coordinate reference system (CRS). For features from Seismic default lines, the clip tool helps to extract the San Francisco Bay regions only. To figure out the population near the default lines, the buffer tool creates boundaries near the fault lines. The boundaries cover through a point buffer. However, the point buffers overlap similar regions, so it is hard to figure out the population by single buffered region. The dissolve tool eliminates the overlapping regions of the point buffer and defines the near default line area. The Join attribute function by location, finally, helps to find the population within the buffered regions. This tool provides the data on the specific population which is vulnerable to earthquakes.\nEarthquake Shaking potential data particularly takes a lot of work to modify the data. This is the actual model for measured earthquake shaking potential by including geological and geographic features information. First, Earthquake shaking potential data doesn’t define the zone area. I divided the five different zones by 1.0-second spectral acceleration (SA10) and scaled 0.4 ranges each. The earthquake shaking potential data was not matched with the geometric way. This data used the fix geometries tool to match with the population data. Afterward, using the by join attributed function by location, the number of people living in each zone can be figured out."
  },
  {
    "objectID": "posts/2022-12-08-QGIS/index.html#results",
    "href": "posts/2022-12-08-QGIS/index.html#results",
    "title": "Earthquake",
    "section": "Results",
    "text": "Results\nOverall population living near seismic fault lines\n\nFigure 3: Buffered the fault lines in San Francisco Bay Area\nTable 1: Population of the San Francisco Bay Area and living near the fault lines\n\nAccording to Figure 2, Santa Clara has the highest population in all of the regions. In Table 1, people live in Santa Clara, which is 25% of all Bay areas, and 39% of the population lives near the fault lines. 54% of people live near the fault lines in Santa Clara.\n\nFigure 4: Graph of Population of San Francisco Bay Area(SFBA) and living near the fault lines\nFigure 4 shows clearer the population of each county. The blue bar represents the people living in each county in San Francisco Bay area and the yellow bar shows the people living near fault lines. Santa Clara shows a high population ratio, a quarter of the total San Francisco Bay Area, and 39 % people in Santa Clara lives near the fault lines. Orange line defines the portion of people near the fault lines each county. The most earthquake affected county is Santa Clara. 54% of people suffered earthquake shaking in this area. The second most earthquake affected area is Napa. Napa is the least people living in Bay Area, but large amount of people lives near the fault lines. Overall, the 34% of people in SFBA live near the fault lines.\nTable 2: Population of living near all fault lines, and San Andreas fault line 　\n\nThe major impact of the potential earthquake comes from San Andreas fault lines. The article discussed the San Andreas rupture along and the strong possibility of joint rupture in the future. (Julian C. Lozos, 2016) In Figure 2, San Andreas pass through Marin, San Mateo and Santa Clara counties. However, Table 2 shows that 91% people in San Mateo live near the San Andreas fault lines.\nThe Actual Model of Earthquake Shaking Potential Risk\n\nFigure 6: Earthquake Shaking Potential in San Francisco Bay Area from NSHM\nThe National Seismic Hazard Maps (NSHM) published the danger of earthquake shaking to see potential risks to avoid people living or working in this area getting hurt. The earthquake slightly moved and new models provide to more accurately define the earthquake shaking regions. In Figure 6, the California Geological Survey (CGS) published the map of “Earthquake Shaking Potential for California” which defines the level of earthquake hazard.\nAccording to D. Branum’s 2015 thesis paper, this actual model extracts from the actual amplified seismic shaking movement. This model explains the potential risk of an earthquake based on the actual ground data. This potential risk is based on the 1.0-second spectral acceleration (SA10), which is expressed as a fraction of gravity. This unit normally used geology and topography that the maximum acceleration in an earthquake on objected special damped.\nBased on the data, earthquake shaking potential is divided into 5 different zones in Table 1. The potential index divides by 0.4 points. Zone 1 is 0 to 0.4, Zone 2 is 0.4 to 0.8, Zone 3 is 0.8 to 1.2, Zone 4 is 1.2 to 1.6, and Zone 5 is over 1.6. If the potential index number is high, the potential risk of an earthquake is high.\nTable 3: population based on the earthquake shaking zone 　\n\nTable 3 shows that Zone 3 and Zone 4 are the major earthquake shaking impact on San Francisco Bay Area. No county includes Zone 1, so it means that there are no safe places in San Francisco Bay Area.\n\nFigure 7: Risk of Potential Earthquake shaking of each zone\nThe bar graph clearly shows each percentage zone of each county. In Figure 7, Alameda and San Mateo shows high portion of Zone 4 and 5. San Mateo is second highest population in San Francisco Bay Area, but high portion of area covers Zone 4 and 5. Therefore, Alameda has the highest risk comparing to the other counties. Additionally, the fault lines don’t pass through San Francisco city, the aftershock and foreshock reach to the city quite high level.\nDiscussion I expected that my hypothesis model defines population near the fault lines data is aligned with the actual model. Near the fault lines expected high potential risk of earthquake and the number of buffered populations is 1.5 times less than combined population in zone 4 and 5 in the actual model.\nIf you compare Figure 2 and Figure 6, is easily recognized the San Andreas fault line at once. In Figure 6, the darkest blue color defines the high potential earthquake shaking risk and it seems to align with the San Andreas fault line. This means the San Andreas has a lot of possible to break the earthquake shaking as we know already.\nThere are some limitations to address the result. First, my census data wasn’t recent data, so the overall population data wasn’t accurate comparing to the present day. Second, my hypothesis model assumes that near the fault line has a high risk of earthquake. However, each fault line has different potential energy or stress level. (Noda,2009) My hypothesis model wasn’t applied to actual movement of earthquake shaking, so data couldn’t show the risk of earthquake."
  },
  {
    "objectID": "posts/2022-12-08-QGIS/index.html#conclusion",
    "href": "posts/2022-12-08-QGIS/index.html#conclusion",
    "title": "Earthquake",
    "section": "Conclusion",
    "text": "Conclusion\nEarthquake shaking carries different kind of potential risk such as landslide, tsunami, and mineral hazard. Mostly, it is major causes of landslides. Landslides happened when rocks or earth material or debris fall down a slope. Landslides occurs anywhere if the condition is proper, and this is reason to damage properties and causalities to people. (Nature) To prevent such damage, it would be necessary to consider such damages into the potential earthquake shaking risk. It would be helped to define more accurate population affected from earthquake shaking."
  },
  {
    "objectID": "posts/2022-12-08-QGIS/index.html#reference",
    "href": "posts/2022-12-08-QGIS/index.html#reference",
    "title": "Earthquake",
    "section": "Reference",
    "text": "Reference\nNational Archives, San Francisco Earthquake (1906), https://www.archives.gov/legislative/features/sf USGS,\nRing of Fire (2012)\nDavid K. Lynch (2006)\nField guide to the San Andreas Fault Julian C. Lozos (2016)\nA case for historic joint rupture of the San Andreas and San Jacinto. faults, Stanford University,https://www.science.org/doi/10.1126/sciadv.1500621 Sandra Hardy (2020)\nGeophysical Investigations of The San Andreas Fault System And Evaluations In Geoscience Education, University of Texas at El Paso San Francisco Chronicle (2021),\nSan Francisco may be small, but it’s among America’s most densely populated cities https://www.sfchronicle.com/sf/article/San-Francisco-may-be- small-but-it-s-among-16650575.php\nHiroyuki Noda, Eric M. Dunham, and James R. Rice(2009), Earthquake ruptures with thermal weakening and the operation of major faults at low overall stress levels, Journal of Geophysical Research, https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.161.4262&rep=rep1&type=pdf\nD. Branum, R. Chen, M. Petersen and C. Wills, (2016),Earthquake Shaking Potential For California Nature, https://www.nature.com/scitable/topicpage/lesson-8-landslides-hazards-8704578/#:~:text=Earthquakes%20are%20a%20major%20cause,casualties%20to%20people%20and%20property."
  }
]